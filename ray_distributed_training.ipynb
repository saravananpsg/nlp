{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyNWZRErkgnxe4/L6OrTb778",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/saravananpsg/nlp/blob/master/ray_distributed_training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E-rNxklCm2WX",
        "outputId": "7cabfb60-4444-4375-a34b-3feba2e62639"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ray\n",
            "  Downloading ray-2.43.0-cp311-cp311-manylinux2014_x86_64.whl.metadata (19 kB)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.11/dist-packages (from ray) (8.1.8)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from ray) (3.17.0)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.11/dist-packages (from ray) (4.23.0)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from ray) (1.1.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from ray) (24.2)\n",
            "Requirement already satisfied: protobuf!=3.19.5,>=3.15.3 in /usr/local/lib/python3.11/dist-packages (from ray) (4.25.6)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from ray) (6.0.2)\n",
            "Requirement already satisfied: aiosignal in /usr/local/lib/python3.11/dist-packages (from ray) (1.3.2)\n",
            "Requirement already satisfied: frozenlist in /usr/local/lib/python3.11/dist-packages (from ray) (1.5.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from ray) (2.32.3)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema->ray) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema->ray) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema->ray) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema->ray) (0.23.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->ray) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->ray) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->ray) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->ray) (2025.1.31)\n",
            "Requirement already satisfied: typing-extensions>=4.4.0 in /usr/local/lib/python3.11/dist-packages (from referencing>=0.28.4->jsonschema->ray) (4.12.2)\n",
            "Downloading ray-2.43.0-cp311-cp311-manylinux2014_x86_64.whl (67.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.7/67.7 MB\u001b[0m \u001b[31m33.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: ray\n",
            "Successfully installed ray-2.43.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "wandb.Api()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        },
        "id": "kj4-9UPasCwq",
        "outputId": "d7f4ba79-dd73-4fb8-9620-d219ad795a47"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "wandb: Paste an API key from your profile and hit enter:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msarapsg\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<wandb.apis.public.api.Api at 0x7cc239c705d0>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import subprocess\n",
        "import socket\n",
        "import time\n",
        "import ray\n",
        "import wandb\n",
        "from datasets import load_dataset, DatasetDict\n",
        "from transformers import (AutoModelForCausalLM, AutoTokenizer, pipeline, TrainingArguments,\n",
        "                          DataCollatorForLanguageModeling)\n",
        "from peft import LoraConfig, get_peft_model, PeftModel\n",
        "from trl import SFTTrainer\n",
        "\n",
        "# Model and Paths\n",
        "MODEL_NAME = \"meta-llama/Llama-2-7b-hf\"  # Replace with your model\n",
        "LORA_SAVE_PATH = \"./lora_adapter\"\n",
        "MERGED_MODEL_PATH = \"./merged_llm\"\n",
        "\n",
        "# ==============================\n",
        "# 1️⃣ Start Ray Cluster\n",
        "# ==============================\n",
        "\n",
        "def get_local_ip():\n",
        "    \"\"\"Get the local IP address of the machine.\"\"\"\n",
        "    return socket.gethostbyname(socket.gethostname())\n",
        "\n",
        "def start_ray_cluster():\n",
        "    \"\"\"Start Ray head node or connect worker nodes dynamically.\"\"\"\n",
        "    local_ip = get_local_ip()\n",
        "    try:\n",
        "        # Try starting Ray as head node\n",
        "        print(\"Starting Ray Head Node...\")\n",
        "        subprocess.run([\"ray\", \"start\", \"--head\"], check=True)\n",
        "        print(\"Ray Head Node started successfully!\")\n",
        "    except subprocess.CalledProcessError:\n",
        "        print(\"Ray Head Node is already running, assuming this is a worker node.\")\n",
        "\n",
        "        # Connect worker node to head node\n",
        "        head_node_ip = local_ip  # You can pass this as an environment variable\n",
        "        print(f\"Connecting to Ray Head Node at {head_node_ip}...\")\n",
        "        subprocess.run([\"ray\", \"start\", f\"--address={head_node_ip}:6379\"], check=True)\n",
        "\n",
        "# Start Ray Cluster\n",
        "start_ray_cluster()\n",
        "\n",
        "# Initialize Ray\n",
        "ray.init(ignore_reinit_error=True)\n",
        "\n",
        "# ==============================\n",
        "# 2️⃣ Load IMDb Dataset\n",
        "# ==============================\n",
        "\n",
        "def load_and_preprocess_data():\n",
        "    \"\"\"Load and preprocess IMDb dataset for causal LM training.\"\"\"\n",
        "    dataset = load_dataset(\"imdb\")\n",
        "\n",
        "    # Combine text and label into a single format\n",
        "    def format_text(example):\n",
        "        return {\"text\": f\"Review: {example['text']}\\nSentiment: {'positive' if example['label'] == 1 else 'negative'}\"}\n",
        "\n",
        "    dataset = dataset.map(format_text, batched=False)\n",
        "\n",
        "    # Train/Test split (keeping only text column)\n",
        "    dataset = DatasetDict({\n",
        "        \"train\": dataset[\"train\"].remove_columns([\"label\"]),\n",
        "        \"test\": dataset[\"test\"].remove_columns([\"label\"]),\n",
        "    })\n",
        "\n",
        "    return dataset\n",
        "\n",
        "# ==============================\n",
        "# 3️⃣ LoRA Training Function\n",
        "# ==============================\n",
        "\n",
        "@ray.remote(num_gpus=1)\n",
        "def train_lora():\n",
        "    wandb.init(project=\"IMDb_LoRA\", entity=\"sarapsg\")  # Replace with your WandB details\n",
        "\n",
        "    # Load dataset\n",
        "    dataset = load_and_preprocess_data()\n",
        "\n",
        "    # Load tokenizer and model\n",
        "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, load_in_8bit=True, device_map=\"auto\")\n",
        "\n",
        "    # LoRA Configuration\n",
        "    lora_config = LoraConfig(\n",
        "        r=16, lora_alpha=32,\n",
        "        lora_dropout=0.1, bias=\"none\"\n",
        "    )\n",
        "    model = get_peft_model(model, lora_config)\n",
        "\n",
        "    # Define training arguments\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=\"./results\",\n",
        "        per_device_train_batch_size=4,\n",
        "        gradient_accumulation_steps=2,\n",
        "        num_train_epochs=1,\n",
        "        learning_rate=2e-4,\n",
        "        fp16=True,\n",
        "        logging_dir=\"./logs\",\n",
        "        remove_unused_columns=False,\n",
        "    )\n",
        "\n",
        "    # Tokenize the dataset\n",
        "    def tokenize_function(examples):\n",
        "        return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=256)\n",
        "\n",
        "    tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "    # Data collator\n",
        "    data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
        "\n",
        "    trainer = SFTTrainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=tokenized_datasets[\"train\"],\n",
        "        tokenizer=tokenizer,\n",
        "        data_collator=data_collator\n",
        "    )\n",
        "\n",
        "    # Train the model\n",
        "    trainer.train()\n",
        "\n",
        "    # Save LoRA adapters\n",
        "    model.save_pretrained(LORA_SAVE_PATH)\n",
        "    tokenizer.save_pretrained(LORA_SAVE_PATH)\n",
        "\n",
        "    # Extract final loss\n",
        "    final_loss = trainer.state.log_history[-1]['loss'] if trainer.state.log_history else None\n",
        "\n",
        "    return final_loss\n",
        "\n",
        "# Run LoRA Training on Multiple GPUs\n",
        "losses = ray.get([train_lora.remote() for _ in range(2)])  # Adjust workers as needed\n",
        "print(\"Final Training Losses from Parallel Runs:\", losses)\n",
        "\n",
        "# ==============================\n",
        "# 4️⃣ Merge LoRA Weights\n",
        "# ==============================\n",
        "\n",
        "@ray.remote(num_gpus=1)\n",
        "def merge_lora():\n",
        "    # Load base model\n",
        "    base_model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, device_map=\"auto\")\n",
        "\n",
        "    # Load trained LoRA adapter\n",
        "    lora_model = PeftModel.from_pretrained(base_model, LORA_SAVE_PATH)\n",
        "\n",
        "    # Merge LoRA weights into base model\n",
        "    merged_model = lora_model.merge_and_unload()\n",
        "\n",
        "    # Save merged model\n",
        "    merged_model.save_pretrained(MERGED_MODEL_PATH)\n",
        "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "    tokenizer.save_pretrained(MERGED_MODEL_PATH)\n",
        "\n",
        "    print(f\"Merged model saved at {MERGED_MODEL_PATH}\")\n",
        "\n",
        "# Run Merging on Distributed GPUs\n",
        "ray.get(merge_lora.remote())\n",
        "\n",
        "# ==============================\n",
        "# 5️⃣ Inference with Merged Model\n",
        "# ==============================\n",
        "\n",
        "def run_inference():\n",
        "    print(\"Loading merged model for inference...\")\n",
        "    merged_pipeline = pipeline(\"text-generation\", model=MERGED_MODEL_PATH)\n",
        "\n",
        "    # Generate text\n",
        "    output = merged_pipeline(\"The movie was\", max_length=50)\n",
        "    print(\"Generated Text:\", output)\n",
        "\n",
        "run_inference()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "HXpjGHxNmpp3",
        "outputId": "19eb7c03-a7c0-421a-bb7c-cd94680a2fb6"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting Ray Head Node...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(train_lora pid=21158)\u001b[0m \r 12%|█▏        | 390/3125 [04:45<33:18,  1.37it/s]\n",
            "\u001b[36m(train_lora pid=21158)\u001b[0m \r 12%|█▏        | 390/3125 [04:45<33:18,  1.37it/s]\n",
            "\u001b[36m(train_lora pid=21158)\u001b[0m \r 12%|█▏        | 390/3125 [04:45<33:18,  1.37it/s]\n",
            "\u001b[36m(train_lora pid=21158)\u001b[0m \r 12%|█▏        | 390/3125 [04:45<33:18,  1.37it/s]\n",
            "\u001b[36m(train_lora pid=21158)\u001b[0m \r 12%|█▏        | 390/3125 [04:45<33:18,  1.37it/s]\n",
            "\u001b[36m(train_lora pid=21158)\u001b[0m \r 12%|█▏        | 390/3125 [04:45<33:18,  1.37it/s]\n",
            "\u001b[36m(train_lora pid=21158)\u001b[0m \r 12%|█▏        | 390/3125 [04:45<33:18,  1.37it/s]\n",
            "\u001b[36m(train_lora pid=21158)\u001b[0m \r 12%|█▏        | 390/3125 [04:45<33:18,  1.37it/s]\n",
            "\u001b[36m(train_lora pid=21158)\u001b[0m \r 12%|█▏        | 390/3125 [04:45<33:18,  1.37it/s]\n",
            "\u001b[36m(train_lora pid=21158)\u001b[0m \r 12%|█▏        | 390/3125 [04:45<33:18,  1.37it/s]\n",
            "\u001b[36m(train_lora pid=21158)\u001b[0m \r 12%|█▏        | 390/3125 [04:45<33:18,  1.37it/s]\n",
            "\u001b[36m(train_lora pid=21158)\u001b[0m \r 12%|█▏        | 390/3125 [04:45<33:18,  1.37it/s]\n",
            "\u001b[36m(train_lora pid=21158)\u001b[0m \r 12%|█▏        | 390/3125 [04:45<33:18,  1.37it/s]\n",
            "\u001b[36m(train_lora pid=21158)\u001b[0m \r 12%|█▏        | 390/3125 [04:45<33:18,  1.37it/s]\n",
            "\u001b[36m(train_lora pid=21158)\u001b[0m \r 12%|█▏        | 390/3125 [04:45<33:18,  1.37it/s]\n",
            "\u001b[36m(train_lora pid=21158)\u001b[0m \r 12%|█▏        | 390/3125 [04:45<33:18,  1.37it/s]\n",
            "\u001b[36m(train_lora pid=21158)\u001b[0m \r 12%|█▏        | 390/3125 [04:45<33:18,  1.37it/s]\n",
            "\u001b[36m(train_lora pid=21158)\u001b[0m \r 12%|█▏        | 390/3125 [04:45<33:18,  1.37it/s]\n",
            "\u001b[36m(train_lora pid=21158)\u001b[0m \r 12%|█▏        | 390/3125 [04:45<33:18,  1.37it/s]\n",
            "\u001b[36m(train_lora pid=21158)\u001b[0m \r 12%|█▏        | 390/3125 [04:45<33:18,  1.37it/s]\n",
            "\u001b[36m(train_lora pid=21158)\u001b[0m \r 12%|█▏        | 390/3125 [04:45<33:18,  1.37it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ray Head Node is already running, assuming this is a worker node.\n",
            "Connecting to Ray Head Node at 172.28.0.12...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(train_lora pid=21158)\u001b[0m \r 13%|█▎        | 391/3125 [04:46<33:18,  1.37it/s]\n",
            "\u001b[36m(train_lora pid=21158)\u001b[0m \r 13%|█▎        | 391/3125 [04:46<33:18,  1.37it/s]\n",
            "\u001b[36m(train_lora pid=21158)\u001b[0m \r 13%|█▎        | 391/3125 [04:46<33:18,  1.37it/s]\n",
            "\u001b[36m(train_lora pid=21158)\u001b[0m \r 13%|█▎        | 391/3125 [04:46<33:18,  1.37it/s]\n",
            "\u001b[36m(train_lora pid=21158)\u001b[0m \r 13%|█▎        | 391/3125 [04:46<33:18,  1.37it/s]\n",
            "\u001b[36m(train_lora pid=21158)\u001b[0m \r 13%|█▎        | 391/3125 [04:46<33:18,  1.37it/s]\n",
            "\u001b[36m(train_lora pid=21158)\u001b[0m \r 13%|█▎        | 391/3125 [04:46<33:18,  1.37it/s]\n",
            "\u001b[36m(train_lora pid=21158)\u001b[0m \r 13%|█▎        | 391/3125 [04:46<33:18,  1.37it/s]\n",
            "\u001b[36m(train_lora pid=21158)\u001b[0m \r 13%|█▎        | 391/3125 [04:46<33:18,  1.37it/s]\n",
            "\u001b[36m(train_lora pid=21158)\u001b[0m \r 13%|█▎        | 391/3125 [04:46<33:18,  1.37it/s]\n",
            "\u001b[36m(train_lora pid=21158)\u001b[0m \r 13%|█▎        | 391/3125 [04:46<33:18,  1.37it/s]\n",
            "\u001b[36m(train_lora pid=21158)\u001b[0m \r 13%|█▎        | 391/3125 [04:46<33:18,  1.37it/s]\n",
            "\u001b[36m(train_lora pid=21158)\u001b[0m \r 13%|█▎        | 391/3125 [04:46<33:18,  1.37it/s]\n",
            "\u001b[36m(train_lora pid=21158)\u001b[0m \r 13%|█▎        | 391/3125 [04:46<33:18,  1.37it/s]\n",
            "\u001b[36m(train_lora pid=21158)\u001b[0m \r 13%|█▎        | 391/3125 [04:46<33:18,  1.37it/s]\n",
            "\u001b[36m(train_lora pid=21158)\u001b[0m \r 13%|█▎        | 391/3125 [04:46<33:18,  1.37it/s]\n",
            "\u001b[36m(train_lora pid=21158)\u001b[0m \r 13%|█▎        | 391/3125 [04:46<33:18,  1.37it/s]\n",
            "\u001b[36m(train_lora pid=21158)\u001b[0m \r 13%|█▎        | 391/3125 [04:46<33:18,  1.37it/s]\n",
            "\u001b[36m(train_lora pid=21158)\u001b[0m \r 13%|█▎        | 391/3125 [04:46<33:18,  1.37it/s]\n",
            "\u001b[36m(train_lora pid=21158)\u001b[0m \r 13%|█▎        | 391/3125 [04:46<33:18,  1.37it/s]\n",
            " 13%|█▎        | 391/3125 [04:46<33:18,  1.37it/s]\n",
            " 13%|█▎        | 392/3125 [04:47<33:20,  1.37it/s]\n",
            " 13%|█▎        | 392/3125 [04:47<33:20,  1.37it/s]\n",
            " 13%|█▎        | 392/3125 [04:47<33:20,  1.37it/s]\n",
            " 13%|█▎        | 392/3125 [04:47<33:20,  1.37it/s]\n",
            " 13%|█▎        | 392/3125 [04:47<33:20,  1.37it/s]\n",
            " 13%|█▎        | 392/3125 [04:47<33:20,  1.37it/s]\n",
            " 13%|█▎        | 392/3125 [04:47<33:20,  1.37it/s]\n",
            " 13%|█▎        | 392/3125 [04:47<33:20,  1.37it/s]\n",
            " 13%|█▎        | 392/3125 [04:47<33:20,  1.37it/s]\n",
            " 13%|█▎        | 392/3125 [04:47<33:20,  1.37it/s]\n",
            " 13%|█▎        | 392/3125 [04:47<33:20,  1.37it/s]\n",
            " 13%|█▎        | 392/3125 [04:47<33:20,  1.37it/s]\n",
            " 13%|█▎        | 392/3125 [04:47<33:20,  1.37it/s]\n",
            " 13%|█▎        | 392/3125 [04:47<33:20,  1.37it/s]\n",
            " 13%|█▎        | 392/3125 [04:47<33:20,  1.37it/s]\n",
            " 13%|█▎        | 392/3125 [04:47<33:20,  1.37it/s]\n",
            " 13%|█▎        | 392/3125 [04:47<33:20,  1.37it/s]\n",
            " 13%|█▎        | 392/3125 [04:47<33:20,  1.37it/s]\n",
            " 13%|█▎        | 392/3125 [04:47<33:20,  1.37it/s]\n",
            " 13%|█▎        | 392/3125 [04:47<33:20,  1.37it/s]\n",
            " 13%|█▎        | 392/3125 [04:47<33:20,  1.37it/s]\n",
            "2025-03-20 04:33:01,820\tINFO worker.py:1654 -- Connecting to existing Ray cluster at address: 172.28.0.12:6379...\n",
            "2025-03-20 04:33:01,821\tINFO worker.py:1672 -- Calling ray.init() again after it has already been called.\n",
            "\u001b[36m(pid=4194)\u001b[0m 2025-03-20 03:50:15.849869: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "\u001b[36m(pid=4194)\u001b[0m WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "\u001b[36m(pid=4194)\u001b[0m E0000 00:00:1742442615.872188    4194 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "\u001b[36m(pid=4194)\u001b[0m E0000 00:00:1742442615.878946    4194 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "\u001b[36m(train_lora pid=4194)\u001b[0m The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
            "Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.31s/it]\n",
            "\u001b[36m(train_lora pid=4194)\u001b[0m <ipython-input-5-d9b0a7069cd6>:87: FutureWarning: `tokenizer` is deprecated and removed starting from version 0.16.0 for `SFTTrainer.__init__`. Use `processing_class` instead.\n",
            "\u001b[36m(train_lora pid=21158)\u001b[0m wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
            "\u001b[36m(train_lora pid=21158)\u001b[0m wandb: Currently logged in as: sarapsg to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\n",
            "\u001b[36m(train_lora pid=21158)\u001b[0m wandb: Tracking run with wandb version 0.19.8\n",
            "\u001b[36m(train_lora pid=21158)\u001b[0m wandb: Run data is saved locally in /content/wandb/run-20250320_042725-eahw6a8f\n",
            "\u001b[36m(train_lora pid=21158)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
            "\u001b[36m(train_lora pid=21158)\u001b[0m wandb: Syncing run mild-music-1\n",
            "\u001b[36m(train_lora pid=21158)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/sarapsg/IMDb_LoRA\n",
            "\u001b[36m(train_lora pid=21158)\u001b[0m wandb: 🚀 View run at https://wandb.ai/sarapsg/IMDb_LoRA/runs/eahw6a8f\n",
            "\u001b[36m(train_lora pid=21158)\u001b[0m Using the latest cached version of the dataset since imdb couldn't be found on the Hugging Face Hub\n",
            "\u001b[36m(train_lora pid=21158)\u001b[0m Found the latest cached dataset configuration 'plain_text' at /root/.cache/huggingface/datasets/imdb/plain_text/0.0.0/e6281661ce1c48d982bc483cf8a173c1bbeb5d31 (last modified on Thu Mar 20 03:43:46 2025).\n",
            "Map: 100%|██████████| 25000/25000 [00:05<00:00, 4624.60 examples/s]\n",
            "Map: 100%|██████████| 25000/25000 [00:05<00:00, 4682.94 examples/s]\n",
            "Map: 100%|██████████| 50000/50000 [00:10<00:00, 4594.56 examples/s]\n",
            "Converting train dataset to ChatML: 100%|██████████| 25000/25000 [00:00<00:00, 25933.73 examples/s]\n",
            "Applying chat template to train dataset: 100%|██████████| 25000/25000 [00:01<00:00, 18266.40 examples/s]\n",
            "Truncating train dataset: 100%|██████████| 25000/25000 [00:05<00:00, 4302.94 examples/s]\n",
            "\u001b[36m(train_lora pid=21158)\u001b[0m wandb: WARNING The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
            " 13%|█▎        | 392/3125 [04:47<33:20,  1.37it/s]\n",
            "Generating train split: 100%|██████████| 25000/25000 [00:00<00:00, 150213.09 examples/s]\n",
            "Generating test split: 100%|██████████| 25000/25000 [00:00<00:00, 201108.94 examples/s]\n",
            "Generating unsupervised split: 100%|██████████| 50000/50000 [00:00<00:00, 200095.60 examples/s]\n",
            "Downloading shards: 100%|██████████| 2/2 [01:03<00:00, 31.58s/it]\n",
            "Converting train dataset to ChatML: 100%|██████████| 3/3 [00:00<00:00, 1262.33 examples/s]\n",
            "Applying chat template to train dataset: 100%|██████████| 3/3 [00:00<00:00, 1267.03 examples/s]\n",
            "Tokenizing train dataset: 100%|██████████| 3/3 [00:00<00:00, 410.66 examples/s]\n",
            "Truncating train dataset: 100%|██████████| 3/3 [00:00<00:00, 848.19 examples/s]\n",
            "\u001b[36m(train_lora pid=4184)\u001b[0m wandb: Syncing run ./results\n",
            "\u001b[36m(train_lora pid=4184)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/sarapsg/huggingface\n",
            "  0%|          | 0/3 [00:00<?, ?it/s]\n",
            "\u001b[36m(train_lora pid=4184)\u001b[0m Exception ignored in atexit callback: <function _start_and_connect_service.<locals>.teardown_atexit at 0x7be53aff3d80>\n",
            "\u001b[36m(train_lora pid=4184)\u001b[0m Traceback (most recent call last):\n",
            "\u001b[36m(train_lora pid=4184)\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/wandb/sdk/lib/service_connection.py\", line 94, in teardown_atexit\n",
            "\u001b[36m(train_lora pid=4184)\u001b[0m     conn.teardown(hooks.exit_code)\n",
            "\u001b[36m(train_lora pid=4184)\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/wandb/sdk/lib/service_connection.py\", line 228, in teardown\n",
            "\u001b[36m(train_lora pid=4184)\u001b[0m     self._client.send_server_request(\n",
            "\u001b[36m(train_lora pid=4184)\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/wandb/sdk/lib/sock_client.py\", line 154, in send_server_request\n",
            "\u001b[36m(train_lora pid=4184)\u001b[0m     self._send_message(msg)\n",
            "\u001b[36m(train_lora pid=4184)\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/wandb/sdk/lib/sock_client.py\", line 151, in _send_message\n",
            "\u001b[36m(train_lora pid=4184)\u001b[0m     self._sendall_with_error_handle(header + data)\n",
            "\u001b[36m(train_lora pid=4184)\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/wandb/sdk/lib/sock_client.py\", line 130, in _sendall_with_error_handle\n",
            "\u001b[36m(train_lora pid=4184)\u001b[0m     sent = self._sock.send(data)\n",
            "\u001b[36m(train_lora pid=4184)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^\n",
            "\u001b[36m(train_lora pid=4184)\u001b[0m BrokenPipeError: [Errno 32] Broken pipe\n",
            "Map:   0%|          | 0/3 [00:00<?, ? examples/s]Asking to pad to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no padding.\n",
            "\u001b[36m(train_lora pid=4189)\u001b[0m Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
            "Map: 100%|██████████| 3/3 [00:00<00:00, 365.22 examples/s]\n",
            "Map:   0%|          | 0/3 [00:00<?, ? examples/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
            "\u001b[36m(train_lora pid=4186)\u001b[0m wandb: 🚀 View run at https://wandb.ai/sarapsg/huggingface/runs/xsbdpnmb\n",
            "\u001b[36m(train_lora pid=16281)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/sarapsg/test\n",
            "Map: 100%|██████████| 3/3 [00:00<00:00, 436.91 examples/s]\n",
            "\u001b[36m(train_lora pid=4190)\u001b[0m message_loop has been closed\n",
            "\u001b[36m(train_lora pid=4190)\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/wandb/sdk/interface/router_sock.py\", line 27, in _read_message\n",
            "\u001b[36m(train_lora pid=4190)\u001b[0m     return self._sock_client.read_server_response(timeout=1)\n",
            "\u001b[36m(train_lora pid=4190)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "\u001b[36m(train_lora pid=4190)\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/wandb/sdk/lib/sock_client.py\", line 235, in read_server_response\n",
            "\u001b[36m(train_lora pid=4190)\u001b[0m     data = self._read_packet_bytes(timeout=timeout)\n",
            "\u001b[36m(train_lora pid=4190)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "\u001b[36m(train_lora pid=4190)\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/wandb/sdk/lib/sock_client.py\", line 220, in _read_packet_bytes\n",
            "\u001b[36m(train_lora pid=4190)\u001b[0m     raise SockClientClosedError\n",
            "\u001b[36m(train_lora pid=4190)\u001b[0m wandb.sdk.lib.sock_client.SockClientClosedError\n",
            "\u001b[36m(train_lora pid=4190)\u001b[0m \n",
            "\u001b[36m(train_lora pid=4190)\u001b[0m The above exception was the direct cause of the following exception:\n",
            "\u001b[36m(train_lora pid=4190)\u001b[0m \n",
            "\u001b[36m(train_lora pid=4190)\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/wandb/sdk/interface/router.py\", line 56, in message_loop\n",
            "\u001b[36m(train_lora pid=4190)\u001b[0m     msg = self._read_message()\n",
            "\u001b[36m(train_lora pid=4190)\u001b[0m           ^^^^^^^^^^^^^^^^^^^^\n",
            "\u001b[36m(train_lora pid=4190)\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/wandb/sdk/interface/router_sock.py\", line 29, in _read_message\n",
            "\u001b[36m(train_lora pid=4190)\u001b[0m     raise MessageRouterClosedError from e\n",
            "\u001b[36m(train_lora pid=4190)\u001b[0m wandb.sdk.interface.router.MessageRouterClosedError\n",
            "100%|██████████| 3/3 [00:02<00:00,  1.13it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(train_lora pid=17751)\u001b[0m {'train_runtime': 2.6691, 'train_samples_per_second': 1.124, 'train_steps_per_second': 1.124, 'train_loss': 4.042674700419108, 'mean_token_accuracy': 0.2777777810891469, 'epoch': 1.0}\n",
            "\u001b[36m(merge_lora pid=17750)\u001b[0m Merged model saved at ./merged_llm\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(train_lora pid=21158)\u001b[0m \r 13%|█▎        | 393/3125 [04:48<37:57,  1.20it/s]\n",
            "\u001b[36m(train_lora pid=21158)\u001b[0m \r 13%|█▎        | 393/3125 [04:48<37:57,  1.20it/s]\n",
            "\u001b[36m(train_lora pid=21158)\u001b[0m \r 13%|█▎        | 393/3125 [04:48<37:57,  1.20it/s]\n",
            "\u001b[36m(train_lora pid=21158)\u001b[0m \r 13%|█▎        | 393/3125 [04:48<37:57,  1.20it/s]\n",
            "\u001b[36m(train_lora pid=21158)\u001b[0m \r 13%|█▎        | 393/3125 [04:48<37:57,  1.20it/s]\n",
            "\u001b[36m(train_lora pid=21158)\u001b[0m \r 13%|█▎        | 393/3125 [04:48<37:57,  1.20it/s]\n",
            "\u001b[36m(train_lora pid=21158)\u001b[0m \r 13%|█▎        | 393/3125 [04:48<37:57,  1.20it/s]\n",
            "\u001b[36m(train_lora pid=21158)\u001b[0m \r 13%|█▎        | 393/3125 [04:48<37:57,  1.20it/s]\n",
            "\u001b[36m(train_lora pid=21158)\u001b[0m \r 13%|█▎        | 393/3125 [04:48<37:57,  1.20it/s]\n",
            "\u001b[36m(train_lora pid=21158)\u001b[0m \r 13%|█▎        | 393/3125 [04:48<37:57,  1.20it/s]\n",
            "\u001b[36m(train_lora pid=21158)\u001b[0m \r 13%|█▎        | 393/3125 [04:48<37:57,  1.20it/s]\n",
            "\u001b[36m(train_lora pid=21158)\u001b[0m \r 13%|█▎        | 393/3125 [04:48<37:57,  1.20it/s]\n",
            "\u001b[36m(train_lora pid=21158)\u001b[0m \r 13%|█▎        | 393/3125 [04:48<37:57,  1.20it/s]\n",
            "\u001b[36m(train_lora pid=21158)\u001b[0m \r 13%|█▎        | 393/3125 [04:48<37:57,  1.20it/s]\n",
            "\u001b[36m(train_lora pid=21158)\u001b[0m \r 13%|█▎        | 393/3125 [04:48<37:57,  1.20it/s]\n",
            "\u001b[36m(train_lora pid=21158)\u001b[0m \r 13%|█▎        | 393/3125 [04:48<37:57,  1.20it/s]\n",
            "\u001b[36m(train_lora pid=21158)\u001b[0m \r 13%|█▎        | 393/3125 [04:48<37:57,  1.20it/s]\n",
            "\u001b[36m(train_lora pid=21158)\u001b[0m \r 13%|█▎        | 393/3125 [04:48<37:57,  1.20it/s]\n",
            "\u001b[36m(train_lora pid=21158)\u001b[0m \r 13%|█▎        | 393/3125 [04:48<37:57,  1.20it/s]\n",
            "\u001b[36m(train_lora pid=21158)\u001b[0m \r 13%|█▎        | 393/3125 [04:48<37:57,  1.20it/s]\n",
            "\u001b[36m(train_lora pid=21158)\u001b[0m \r 13%|█▎        | 393/3125 [04:48<37:57,  1.20it/s]\n",
            " 13%|█▎        | 393/3125 [04:48<37:57,  1.20it/s]\n",
            " 13%|█▎        | 394/3125 [04:49<39:30,  1.15it/s]\n",
            " 13%|█▎        | 394/3125 [04:49<39:30,  1.15it/s]\n",
            " 13%|█▎        | 394/3125 [04:49<39:30,  1.15it/s]\n",
            " 13%|█▎        | 394/3125 [04:49<39:30,  1.15it/s]\n",
            " 13%|█▎        | 394/3125 [04:49<39:30,  1.15it/s]\n",
            " 13%|█▎        | 394/3125 [04:49<39:30,  1.15it/s]\n",
            " 13%|█▎        | 394/3125 [04:49<39:30,  1.15it/s]\n",
            " 13%|█▎        | 394/3125 [04:49<39:30,  1.15it/s]\n",
            " 13%|█▎        | 394/3125 [04:49<39:30,  1.15it/s]\n",
            " 13%|█▎        | 394/3125 [04:49<39:30,  1.15it/s]\n",
            " 13%|█▎        | 394/3125 [04:49<39:30,  1.15it/s]\n",
            " 13%|█▎        | 394/3125 [04:49<39:30,  1.15it/s]\n",
            " 13%|█▎        | 394/3125 [04:49<39:30,  1.15it/s]\n",
            " 13%|█▎        | 394/3125 [04:49<39:30,  1.15it/s]\n",
            " 13%|█▎        | 394/3125 [04:49<39:30,  1.15it/s]\n",
            " 13%|█▎        | 394/3125 [04:49<39:30,  1.15it/s]\n",
            " 13%|█▎        | 394/3125 [04:49<39:30,  1.15it/s]\n",
            " 13%|█▎        | 394/3125 [04:49<39:30,  1.15it/s]\n",
            " 13%|█▎        | 394/3125 [04:49<39:30,  1.15it/s]\n",
            " 13%|█▎        | 394/3125 [04:49<39:30,  1.15it/s]\n",
            " 13%|█▎        | 394/3125 [04:49<39:30,  1.15it/s]\n",
            " 13%|█▎        | 394/3125 [04:49<39:30,  1.15it/s]\n",
            " 13%|█▎        | 395/3125 [04:50<38:44,  1.17it/s]\n",
            " 13%|█▎        | 395/3125 [04:50<38:44,  1.17it/s]\n",
            " 13%|█▎        | 395/3125 [04:50<38:44,  1.17it/s]\n",
            " 13%|█▎        | 395/3125 [04:50<38:44,  1.17it/s]\n",
            " 13%|█▎        | 395/3125 [04:50<38:44,  1.17it/s]\n",
            " 13%|█▎        | 395/3125 [04:50<38:44,  1.17it/s]\n",
            " 13%|█▎        | 395/3125 [04:50<38:44,  1.17it/s]\n",
            " 13%|█▎        | 395/3125 [04:50<38:44,  1.17it/s]\n",
            " 13%|█▎        | 395/3125 [04:50<38:44,  1.17it/s]\n",
            " 13%|█▎        | 395/3125 [04:50<38:44,  1.17it/s]\n",
            " 13%|█▎        | 395/3125 [04:50<38:44,  1.17it/s]\n",
            " 13%|█▎        | 395/3125 [04:50<38:44,  1.17it/s]\n",
            " 13%|█▎        | 395/3125 [04:50<38:44,  1.17it/s]\n",
            " 13%|█▎        | 395/3125 [04:50<38:44,  1.17it/s]\n",
            " 13%|█▎        | 395/3125 [04:50<38:44,  1.17it/s]\n",
            " 13%|█▎        | 395/3125 [04:50<38:44,  1.17it/s]\n",
            " 13%|█▎        | 395/3125 [04:50<38:44,  1.17it/s]\n",
            " 13%|█▎        | 395/3125 [04:50<38:44,  1.17it/s]\n",
            " 13%|█▎        | 395/3125 [04:50<38:44,  1.17it/s]\n",
            " 13%|█▎        | 395/3125 [04:50<38:44,  1.17it/s]\n",
            " 13%|█▎        | 395/3125 [04:50<38:44,  1.17it/s]\n",
            " 13%|█▎        | 395/3125 [04:50<38:44,  1.17it/s]\n",
            " 13%|█▎        | 396/3125 [04:50<38:22,  1.19it/s]\n",
            " 13%|█▎        | 396/3125 [04:50<38:22,  1.19it/s]\n",
            " 13%|█▎        | 396/3125 [04:50<38:22,  1.19it/s]\n",
            " 13%|█▎        | 396/3125 [04:50<38:22,  1.19it/s]\n",
            " 13%|█▎        | 396/3125 [04:50<38:22,  1.19it/s]\n",
            " 13%|█▎        | 396/3125 [04:50<38:22,  1.19it/s]\n",
            " 13%|█▎        | 396/3125 [04:50<38:22,  1.19it/s]\n",
            " 13%|█▎        | 396/3125 [04:50<38:22,  1.19it/s]\n",
            " 13%|█▎        | 396/3125 [04:50<38:22,  1.19it/s]\n",
            " 13%|█▎        | 396/3125 [04:50<38:22,  1.19it/s]\n",
            " 13%|█▎        | 396/3125 [04:50<38:22,  1.19it/s]\n",
            " 13%|█▎        | 396/3125 [04:50<38:22,  1.19it/s]\n",
            " 13%|█▎        | 396/3125 [04:50<38:22,  1.19it/s]\n",
            " 13%|█▎        | 396/3125 [04:50<38:22,  1.19it/s]\n",
            " 13%|█▎        | 396/3125 [04:50<38:22,  1.19it/s]\n",
            " 13%|█▎        | 396/3125 [04:50<38:22,  1.19it/s]\n",
            " 13%|█▎        | 396/3125 [04:50<38:22,  1.19it/s]\n",
            " 13%|█▎        | 396/3125 [04:50<38:22,  1.19it/s]\n",
            " 13%|█▎        | 396/3125 [04:50<38:22,  1.19it/s]\n",
            " 13%|█▎        | 396/3125 [04:50<38:22,  1.19it/s]\n",
            " 13%|█▎        | 396/3125 [04:50<38:22,  1.19it/s]\n",
            " 13%|█▎        | 396/3125 [04:50<38:22,  1.19it/s]\n",
            " 13%|█▎        | 397/3125 [04:51<37:49,  1.20it/s]\n",
            " 13%|█▎        | 397/3125 [04:51<37:49,  1.20it/s]\n",
            " 13%|█▎        | 397/3125 [04:51<37:49,  1.20it/s]\n",
            " 13%|█▎        | 397/3125 [04:51<37:49,  1.20it/s]\n",
            " 13%|█▎        | 397/3125 [04:51<37:49,  1.20it/s]\n",
            " 13%|█▎        | 397/3125 [04:51<37:49,  1.20it/s]\n",
            " 13%|█▎        | 397/3125 [04:51<37:49,  1.20it/s]\n",
            " 13%|█▎        | 397/3125 [04:51<37:49,  1.20it/s]\n",
            " 13%|█▎        | 397/3125 [04:51<37:49,  1.20it/s]\n",
            " 13%|█▎        | 397/3125 [04:51<37:49,  1.20it/s]\n",
            " 13%|█▎        | 397/3125 [04:51<37:49,  1.20it/s]\n",
            " 13%|█▎        | 397/3125 [04:51<37:49,  1.20it/s]\n",
            " 13%|█▎        | 397/3125 [04:51<37:49,  1.20it/s]\n",
            " 13%|█▎        | 397/3125 [04:51<37:49,  1.20it/s]\n",
            " 13%|█▎        | 397/3125 [04:51<37:49,  1.20it/s]\n",
            " 13%|█▎        | 397/3125 [04:51<37:49,  1.20it/s]\n",
            " 13%|█▎        | 397/3125 [04:51<37:49,  1.20it/s]\n",
            " 13%|█▎        | 397/3125 [04:51<37:49,  1.20it/s]\n",
            " 13%|█▎        | 397/3125 [04:51<37:49,  1.20it/s]\n",
            " 13%|█▎        | 397/3125 [04:51<37:49,  1.20it/s]\n",
            " 13%|█▎        | 397/3125 [04:51<37:49,  1.20it/s]\n",
            " 13%|█▎        | 397/3125 [04:51<37:49,  1.20it/s]\n",
            " 13%|█▎        | 398/3125 [04:52<36:47,  1.24it/s]\n",
            " 13%|█▎        | 398/3125 [04:52<36:47,  1.24it/s]\n",
            " 13%|█▎        | 398/3125 [04:52<36:47,  1.24it/s]\n",
            " 13%|█▎        | 398/3125 [04:52<36:47,  1.24it/s]\n",
            " 13%|█▎        | 398/3125 [04:52<36:47,  1.24it/s]\n",
            " 13%|█▎        | 398/3125 [04:52<36:47,  1.24it/s]\n",
            " 13%|█▎        | 398/3125 [04:52<36:47,  1.24it/s]\n",
            " 13%|█▎        | 398/3125 [04:52<36:47,  1.24it/s]\n",
            " 13%|█▎        | 398/3125 [04:52<36:47,  1.24it/s]\n",
            " 13%|█▎        | 398/3125 [04:52<36:47,  1.24it/s]\n",
            " 13%|█▎        | 398/3125 [04:52<36:47,  1.24it/s]\n",
            " 13%|█▎        | 398/3125 [04:52<36:47,  1.24it/s]\n",
            " 13%|█▎        | 398/3125 [04:52<36:47,  1.24it/s]\n",
            " 13%|█▎        | 398/3125 [04:52<36:47,  1.24it/s]\n",
            " 13%|█▎        | 398/3125 [04:52<36:47,  1.24it/s]\n",
            " 13%|█▎        | 398/3125 [04:52<36:47,  1.24it/s]\n",
            " 13%|█▎        | 398/3125 [04:52<36:47,  1.24it/s]\n",
            " 13%|█▎        | 398/3125 [04:52<36:47,  1.24it/s]\n",
            " 13%|█▎        | 398/3125 [04:52<36:47,  1.24it/s]\n",
            " 13%|█▎        | 398/3125 [04:52<36:47,  1.24it/s]\n",
            " 13%|█▎        | 398/3125 [04:52<36:47,  1.24it/s]\n",
            " 13%|█▎        | 398/3125 [04:52<36:47,  1.24it/s]\n",
            " 13%|█▎        | 399/3125 [04:53<36:11,  1.26it/s]\n",
            "\u001b[36m(pid=4193)\u001b[0m 2025-03-20 04:01:13.529453: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\u001b[32m [repeated 22x across cluster]\u001b[0m\n",
            "\u001b[36m(pid=4193)\u001b[0m WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\u001b[32m [repeated 22x across cluster]\u001b[0m\n",
            "\u001b[36m(pid=4193)\u001b[0m E0000 00:00:1742443273.552946    4193 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\u001b[32m [repeated 22x across cluster]\u001b[0m\n",
            "\u001b[36m(pid=4193)\u001b[0m E0000 00:00:1742443273.559851    4193 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\u001b[32m [repeated 22x across cluster]\u001b[0m\n",
            "\u001b[36m(train_lora pid=4193)\u001b[0m The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\u001b[32m [repeated 20x across cluster]\u001b[0m\n",
            "Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.32s/it]\u001b[32m [repeated 21x across cluster]\u001b[0m\n",
            "\u001b[36m(train_lora pid=17751)\u001b[0m <ipython-input-26-73d65f9bed43>:104: FutureWarning: `tokenizer` is deprecated and removed starting from version 0.16.0 for `SFTTrainer.__init__`. Use `processing_class` instead.\u001b[32m [repeated 14x across cluster]\u001b[0m\n",
            "\u001b[36m(train_lora pid=17751)\u001b[0m wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\u001b[32m [repeated 12x across cluster]\u001b[0m\n",
            "\u001b[36m(train_lora pid=17751)\u001b[0m wandb: Currently logged in as: sarapsg to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\u001b[32m [repeated 11x across cluster]\u001b[0m\n",
            "\u001b[36m(train_lora pid=17751)\u001b[0m wandb: Tracking run with wandb version 0.19.8\u001b[32m [repeated 11x across cluster]\u001b[0m\n",
            "\u001b[36m(train_lora pid=17751)\u001b[0m wandb: Run data is saved locally in /content/wandb/run-20250320_041505-b1w4fxhx\u001b[32m [repeated 11x across cluster]\u001b[0m\n",
            "\u001b[36m(train_lora pid=17751)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\u001b[32m [repeated 11x across cluster]\u001b[0m\n",
            "\u001b[36m(train_lora pid=17751)\u001b[0m wandb: Syncing run glowing-grass-3\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(train_lora pid=17751)\u001b[0m wandb: 🚀 View run at https://wandb.ai/sarapsg/test/runs/b1w4fxhx\u001b[32m [repeated 10x across cluster]\u001b[0m\n",
            "\u001b[36m(train_lora pid=17751)\u001b[0m wandb: WARNING The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\u001b[32m [repeated 12x across cluster]\u001b[0m\n",
            "Downloading shards: 100%|██████████| 2/2 [01:03<00:00, 31.63s/it]\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "Converting train dataset to ChatML: 100%|██████████| 3/3 [00:00<00:00, 874.97 examples/s]\u001b[32m [repeated 11x across cluster]\u001b[0m\n",
            "Applying chat template to train dataset: 100%|██████████| 3/3 [00:00<00:00, 839.98 examples/s]\u001b[32m [repeated 11x across cluster]\u001b[0m\n",
            "Tokenizing train dataset: 100%|██████████| 3/3 [00:00<00:00, 438.96 examples/s]\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "Truncating train dataset: 100%|██████████| 3/3 [00:00<00:00, 600.19 examples/s]\u001b[32m [repeated 11x across cluster]\u001b[0m\n",
            "\u001b[36m(train_lora pid=16280)\u001b[0m wandb: Syncing run ./results\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
            "\u001b[36m(train_lora pid=16280)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/sarapsg/huggingface\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
            "  0%|          | 0/3 [00:00<?, ?it/s]\u001b[32m [repeated 9x across cluster]\u001b[0m\n",
            "\u001b[36m(train_lora pid=16280)\u001b[0m Exception ignored in atexit callback: <function _start_and_connect_service.<locals>.teardown_atexit at 0x7e3cb62a22a0>\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
            "\u001b[36m(train_lora pid=16280)\u001b[0m Traceback (most recent call last):\u001b[32m [repeated 10x across cluster]\u001b[0m\n",
            "\u001b[36m(train_lora pid=16280)\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/wandb/sdk/lib/service_connection.py\", line 94, in teardown_atexit\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
            "\u001b[36m(train_lora pid=16280)\u001b[0m     conn.teardown(hooks.exit_code)\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
            "\u001b[36m(train_lora pid=16280)\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/wandb/sdk/lib/service_connection.py\", line 228, in teardown\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
            "\u001b[36m(train_lora pid=16280)\u001b[0m     self._client.send_server_request(\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
            "\u001b[36m(train_lora pid=16280)\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/wandb/sdk/lib/sock_client.py\", line 154, in send_server_request\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
            "\u001b[36m(train_lora pid=16280)\u001b[0m     self._send_message(msg)\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
            "\u001b[36m(train_lora pid=16280)\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/wandb/sdk/lib/sock_client.py\", line 151, in _send_message\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
            "\u001b[36m(train_lora pid=16280)\u001b[0m     self._sendall_with_error_handle(header + data)\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
            "\u001b[36m(train_lora pid=16280)\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/wandb/sdk/lib/sock_client.py\", line 130, in _sendall_with_error_handle\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
            "\u001b[36m(train_lora pid=16280)\u001b[0m     sent = self._sock.send(data)\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
            "\u001b[36m(train_lora pid=16280)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
            "\u001b[36m(train_lora pid=16280)\u001b[0m BrokenPipeError: [Errno 32] Broken pipe\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
            "Map:   0%|          | 0/3 [00:00<?, ? examples/s]Asking to pad to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no padding.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(train_lora pid=13753)\u001b[0m Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "Map: 100%|██████████| 3/3 [00:00<00:00, 307.62 examples/s]\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
            "\u001b[36m(train_lora pid=17751)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/sarapsg/test\n",
            "Map: 100%|██████████| 3/3 [00:00<00:00, 390.28 examples/s]\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
            " 13%|█▎        | 399/3125 [04:53<36:11,  1.26it/s]\n",
            " 13%|█▎        | 399/3125 [04:53<36:11,  1.26it/s]\n",
            " 13%|█▎        | 399/3125 [04:53<36:11,  1.26it/s]\n",
            " 13%|█▎        | 399/3125 [04:53<36:11,  1.26it/s]\n",
            " 13%|█▎        | 399/3125 [04:53<36:11,  1.26it/s]\n",
            " 13%|█▎        | 399/3125 [04:53<36:11,  1.26it/s]\n",
            " 13%|█▎        | 399/3125 [04:53<36:11,  1.26it/s]\n",
            " 13%|█▎        | 399/3125 [04:53<36:11,  1.26it/s]\n",
            " 13%|█▎        | 399/3125 [04:53<36:11,  1.26it/s]\n",
            " 13%|█▎        | 399/3125 [04:53<36:11,  1.26it/s]\n",
            " 13%|█▎        | 399/3125 [04:53<36:11,  1.26it/s]\n",
            " 13%|█▎        | 399/3125 [04:53<36:11,  1.26it/s]\n",
            " 13%|█▎        | 399/3125 [04:53<36:11,  1.26it/s]\n",
            " 13%|█▎        | 399/3125 [04:53<36:11,  1.26it/s]\n",
            " 13%|█▎        | 399/3125 [04:53<36:11,  1.26it/s]\n",
            " 13%|█▎        | 399/3125 [04:53<36:11,  1.26it/s]\n",
            " 13%|█▎        | 399/3125 [04:53<36:11,  1.26it/s]\n",
            " 13%|█▎        | 399/3125 [04:53<36:11,  1.26it/s]\n",
            " 13%|█▎        | 399/3125 [04:53<36:11,  1.26it/s]\n",
            " 13%|█▎        | 399/3125 [04:53<36:11,  1.26it/s]\n",
            " 13%|█▎        | 399/3125 [04:53<36:11,  1.26it/s]\n",
            " 13%|█▎        | 400/3125 [04:53<35:42,  1.27it/s]\n",
            " 13%|█▎        | 400/3125 [04:53<35:42,  1.27it/s]\n",
            " 13%|█▎        | 400/3125 [04:53<35:42,  1.27it/s]\n",
            " 13%|█▎        | 400/3125 [04:53<35:42,  1.27it/s]\n",
            " 13%|█▎        | 400/3125 [04:53<35:42,  1.27it/s]\n",
            " 13%|█▎        | 400/3125 [04:53<35:42,  1.27it/s]\n",
            " 13%|█▎        | 400/3125 [04:53<35:42,  1.27it/s]\n",
            " 13%|█▎        | 400/3125 [04:53<35:42,  1.27it/s]\n",
            " 13%|█▎        | 400/3125 [04:53<35:42,  1.27it/s]\n",
            " 13%|█▎        | 400/3125 [04:53<35:42,  1.27it/s]\n",
            " 13%|█▎        | 400/3125 [04:53<35:42,  1.27it/s]\n",
            " 13%|█▎        | 400/3125 [04:53<35:42,  1.27it/s]\n",
            " 13%|█▎        | 400/3125 [04:53<35:42,  1.27it/s]\n",
            " 13%|█▎        | 400/3125 [04:53<35:42,  1.27it/s]\n",
            " 13%|█▎        | 400/3125 [04:53<35:42,  1.27it/s]\n",
            " 13%|█▎        | 400/3125 [04:53<35:42,  1.27it/s]\n",
            " 13%|█▎        | 400/3125 [04:53<35:42,  1.27it/s]\n",
            " 13%|█▎        | 400/3125 [04:53<35:42,  1.27it/s]\n",
            " 13%|█▎        | 400/3125 [04:53<35:42,  1.27it/s]\n",
            " 13%|█▎        | 400/3125 [04:53<35:42,  1.27it/s]\n",
            " 13%|█▎        | 400/3125 [04:53<35:42,  1.27it/s]\n",
            " 13%|█▎        | 400/3125 [04:53<35:42,  1.27it/s]\n",
            " 13%|█▎        | 401/3125 [04:54<35:21,  1.28it/s]\n",
            " 13%|█▎        | 401/3125 [04:54<35:21,  1.28it/s]\n",
            " 13%|█▎        | 401/3125 [04:54<35:21,  1.28it/s]\n",
            " 13%|█▎        | 401/3125 [04:54<35:21,  1.28it/s]\n",
            " 13%|█▎        | 401/3125 [04:54<35:21,  1.28it/s]\n",
            " 13%|█▎        | 401/3125 [04:54<35:21,  1.28it/s]\n",
            " 13%|█▎        | 401/3125 [04:54<35:21,  1.28it/s]\n",
            " 13%|█▎        | 401/3125 [04:54<35:21,  1.28it/s]\n",
            " 13%|█▎        | 401/3125 [04:54<35:21,  1.28it/s]\n",
            " 13%|█▎        | 401/3125 [04:54<35:21,  1.28it/s]\n",
            " 13%|█▎        | 401/3125 [04:54<35:21,  1.28it/s]\n",
            " 13%|█▎        | 401/3125 [04:54<35:21,  1.28it/s]\n",
            " 13%|█▎        | 401/3125 [04:54<35:21,  1.28it/s]\n",
            " 13%|█▎        | 401/3125 [04:54<35:21,  1.28it/s]\n",
            " 13%|█▎        | 401/3125 [04:54<35:21,  1.28it/s]\n",
            " 13%|█▎        | 401/3125 [04:54<35:21,  1.28it/s]\n",
            " 13%|█▎        | 401/3125 [04:54<35:21,  1.28it/s]\n",
            " 13%|█▎        | 401/3125 [04:54<35:21,  1.28it/s]\n",
            " 13%|█▎        | 401/3125 [04:54<35:21,  1.28it/s]\n",
            " 13%|█▎        | 401/3125 [04:54<35:21,  1.28it/s]\n",
            " 13%|█▎        | 401/3125 [04:54<35:21,  1.28it/s]\n",
            " 13%|█▎        | 401/3125 [04:54<35:21,  1.28it/s]\n",
            " 13%|█▎        | 402/3125 [04:55<35:09,  1.29it/s]\n",
            " 13%|█▎        | 402/3125 [04:55<35:09,  1.29it/s]\n",
            " 13%|█▎        | 402/3125 [04:55<35:09,  1.29it/s]\n",
            " 13%|█▎        | 402/3125 [04:55<35:09,  1.29it/s]\n",
            " 13%|█▎        | 402/3125 [04:55<35:09,  1.29it/s]\n",
            " 13%|█▎        | 402/3125 [04:55<35:09,  1.29it/s]\n",
            " 13%|█▎        | 402/3125 [04:55<35:09,  1.29it/s]\n",
            " 13%|█▎        | 402/3125 [04:55<35:09,  1.29it/s]\n",
            " 13%|█▎        | 402/3125 [04:55<35:09,  1.29it/s]\n",
            " 13%|█▎        | 402/3125 [04:55<35:09,  1.29it/s]\n",
            " 13%|█▎        | 402/3125 [04:55<35:09,  1.29it/s]\n",
            " 13%|█▎        | 402/3125 [04:55<35:09,  1.29it/s]\n",
            " 13%|█▎        | 402/3125 [04:55<35:09,  1.29it/s]\n",
            " 13%|█▎        | 402/3125 [04:55<35:09,  1.29it/s]\n",
            " 13%|█▎        | 402/3125 [04:55<35:09,  1.29it/s]\n",
            " 13%|█▎        | 402/3125 [04:55<35:09,  1.29it/s]\n",
            " 13%|█▎        | 402/3125 [04:55<35:09,  1.29it/s]\n",
            " 13%|█▎        | 402/3125 [04:55<35:09,  1.29it/s]\n",
            " 13%|█▎        | 402/3125 [04:55<35:09,  1.29it/s]\n",
            " 13%|█▎        | 402/3125 [04:55<35:09,  1.29it/s]\n",
            " 13%|█▎        | 402/3125 [04:55<35:09,  1.29it/s]\n",
            " 13%|█▎        | 402/3125 [04:55<35:09,  1.29it/s]\n",
            " 13%|█▎        | 403/3125 [04:56<34:50,  1.30it/s]\n",
            " 13%|█▎        | 403/3125 [04:56<34:50,  1.30it/s]\n",
            " 13%|█▎        | 403/3125 [04:56<34:50,  1.30it/s]\n",
            " 13%|█▎        | 403/3125 [04:56<34:50,  1.30it/s]\n",
            " 13%|█▎        | 403/3125 [04:56<34:50,  1.30it/s]\n",
            " 13%|█▎        | 403/3125 [04:56<34:50,  1.30it/s]\n",
            " 13%|█▎        | 403/3125 [04:56<34:50,  1.30it/s]\n",
            " 13%|█▎        | 403/3125 [04:56<34:50,  1.30it/s]\n",
            " 13%|█▎        | 403/3125 [04:56<34:50,  1.30it/s]\n",
            " 13%|█▎        | 403/3125 [04:56<34:50,  1.30it/s]\n",
            " 13%|█▎        | 403/3125 [04:56<34:50,  1.30it/s]\n",
            " 13%|█▎        | 403/3125 [04:56<34:50,  1.30it/s]\n",
            " 13%|█▎        | 403/3125 [04:56<34:50,  1.30it/s]\n",
            " 13%|█▎        | 403/3125 [04:56<34:50,  1.30it/s]\n",
            " 13%|█▎        | 403/3125 [04:56<34:50,  1.30it/s]\n",
            " 13%|█▎        | 403/3125 [04:56<34:50,  1.30it/s]\n",
            " 13%|█▎        | 403/3125 [04:56<34:50,  1.30it/s]\n",
            " 13%|█▎        | 403/3125 [04:56<34:50,  1.30it/s]\n",
            " 13%|█▎        | 403/3125 [04:56<34:50,  1.30it/s]\n",
            " 13%|█▎        | 403/3125 [04:56<34:50,  1.30it/s]\n",
            " 13%|█▎        | 403/3125 [04:56<34:50,  1.30it/s]\n",
            " 13%|█▎        | 403/3125 [04:56<34:50,  1.30it/s]\n",
            " 13%|█▎        | 404/3125 [04:56<34:23,  1.32it/s]\n",
            " 13%|█▎        | 404/3125 [04:56<34:23,  1.32it/s]\n",
            " 13%|█▎        | 404/3125 [04:56<34:23,  1.32it/s]\n",
            " 13%|█▎        | 404/3125 [04:56<34:23,  1.32it/s]\n",
            " 13%|█▎        | 404/3125 [04:56<34:23,  1.32it/s]\n",
            " 13%|█▎        | 404/3125 [04:56<34:23,  1.32it/s]\n",
            " 13%|█▎        | 404/3125 [04:56<34:23,  1.32it/s]\n",
            " 13%|█▎        | 404/3125 [04:56<34:23,  1.32it/s]\n",
            " 13%|█▎        | 404/3125 [04:56<34:23,  1.32it/s]\n",
            " 13%|█▎        | 404/3125 [04:56<34:23,  1.32it/s]\n",
            " 13%|█▎        | 404/3125 [04:56<34:23,  1.32it/s]\n",
            " 13%|█▎        | 404/3125 [04:56<34:23,  1.32it/s]\n",
            " 13%|█▎        | 404/3125 [04:56<34:23,  1.32it/s]\n",
            " 13%|█▎        | 404/3125 [04:56<34:23,  1.32it/s]\n",
            " 13%|█▎        | 404/3125 [04:56<34:23,  1.32it/s]\n",
            " 13%|█▎        | 404/3125 [04:56<34:23,  1.32it/s]\n",
            " 13%|█▎        | 404/3125 [04:56<34:23,  1.32it/s]\n",
            " 13%|█▎        | 404/3125 [04:56<34:23,  1.32it/s]\n",
            " 13%|█▎        | 404/3125 [04:56<34:23,  1.32it/s]\n",
            " 13%|█▎        | 404/3125 [04:56<34:23,  1.32it/s]\n",
            " 13%|█▎        | 404/3125 [04:56<34:23,  1.32it/s]\n",
            " 13%|█▎        | 404/3125 [04:56<34:23,  1.32it/s]\n",
            " 13%|█▎        | 405/3125 [04:57<34:07,  1.33it/s]\n",
            " 13%|█▎        | 405/3125 [04:57<34:07,  1.33it/s]\n",
            " 13%|█▎        | 405/3125 [04:57<34:07,  1.33it/s]\n",
            " 13%|█▎        | 405/3125 [04:57<34:07,  1.33it/s]\n",
            " 13%|█▎        | 405/3125 [04:57<34:07,  1.33it/s]\n",
            " 13%|█▎        | 405/3125 [04:57<34:07,  1.33it/s]\n",
            " 13%|█▎        | 405/3125 [04:57<34:07,  1.33it/s]\n",
            " 13%|█▎        | 405/3125 [04:57<34:07,  1.33it/s]\n",
            " 13%|█▎        | 405/3125 [04:57<34:07,  1.33it/s]\n",
            " 13%|█▎        | 405/3125 [04:57<34:07,  1.33it/s]\n",
            " 13%|█▎        | 405/3125 [04:57<34:07,  1.33it/s]\n",
            " 13%|█▎        | 405/3125 [04:57<34:07,  1.33it/s]\n",
            " 13%|█▎        | 405/3125 [04:57<34:07,  1.33it/s]\n",
            " 13%|█▎        | 405/3125 [04:57<34:07,  1.33it/s]\n",
            " 13%|█▎        | 405/3125 [04:57<34:07,  1.33it/s]\n",
            " 13%|█▎        | 405/3125 [04:57<34:07,  1.33it/s]\n",
            " 13%|█▎        | 405/3125 [04:57<34:07,  1.33it/s]\n",
            " 13%|█▎        | 405/3125 [04:57<34:07,  1.33it/s]\n",
            " 13%|█▎        | 405/3125 [04:57<34:07,  1.33it/s]\n",
            " 13%|█▎        | 405/3125 [04:57<34:07,  1.33it/s]\n",
            " 13%|█▎        | 405/3125 [04:57<34:07,  1.33it/s]\n",
            " 13%|█▎        | 405/3125 [04:57<34:07,  1.33it/s]\n",
            "\u001b[36m(train_lora pid=22919)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/sarapsg/IMDb_LoRA\n",
            "\u001b[36m(train_lora pid=22919)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/sarapsg/IMDb_LoRA\n",
            "\u001b[36m(train_lora pid=22919)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/sarapsg/IMDb_LoRA\n",
            "\u001b[36m(train_lora pid=22919)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/sarapsg/IMDb_LoRA\n",
            "\u001b[36m(train_lora pid=22919)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/sarapsg/IMDb_LoRA\n",
            "\u001b[36m(train_lora pid=22919)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/sarapsg/IMDb_LoRA\n",
            "\u001b[36m(train_lora pid=22919)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/sarapsg/IMDb_LoRA\n",
            "\u001b[36m(train_lora pid=22919)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/sarapsg/IMDb_LoRA\n",
            "\u001b[36m(train_lora pid=22919)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/sarapsg/IMDb_LoRA\n",
            "\u001b[36m(train_lora pid=22919)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/sarapsg/IMDb_LoRA\n",
            "\u001b[36m(train_lora pid=22919)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/sarapsg/IMDb_LoRA\n",
            "\u001b[36m(pid=22917)\u001b[0m 2025-03-20 04:33:09.301688: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\u001b[32m [repeated 44x across cluster]\u001b[0m\n",
            "\u001b[36m(pid=22917)\u001b[0m WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\u001b[32m [repeated 44x across cluster]\u001b[0m\n",
            "\u001b[36m(pid=22917)\u001b[0m E0000 00:00:1742445189.325046   22917 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\u001b[32m [repeated 44x across cluster]\u001b[0m\n",
            "\u001b[36m(pid=22917)\u001b[0m E0000 00:00:1742445189.332105   22917 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\u001b[32m [repeated 44x across cluster]\u001b[0m\n",
            "\u001b[36m(train_lora pid=22919)\u001b[0m wandb: Currently logged in as: sarapsg to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\u001b[32m [repeated 44x across cluster]\u001b[0m\n",
            "\u001b[36m(train_lora pid=22919)\u001b[0m wandb: Tracking run with wandb version 0.19.8\u001b[32m [repeated 11x across cluster]\u001b[0m\n",
            "\u001b[36m(train_lora pid=22919)\u001b[0m wandb: Run data is saved locally in /content/wandb/run-20250320_043312-gg2gj5uu\u001b[32m [repeated 11x across cluster]\u001b[0m\n",
            "\u001b[36m(train_lora pid=22919)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\u001b[32m [repeated 11x across cluster]\u001b[0m\n",
            "\u001b[36m(train_lora pid=22919)\u001b[0m wandb: Syncing run lilac-voice-2\u001b[32m [repeated 11x across cluster]\u001b[0m\n",
            "\u001b[36m(train_lora pid=22919)\u001b[0m wandb: 🚀 View run at https://wandb.ai/sarapsg/IMDb_LoRA/runs/gg2gj5uu\u001b[32m [repeated 11x across cluster]\u001b[0m\n",
            "\u001b[36m(train_lora pid=22919)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/sarapsg/IMDb_LoRA\n",
            "\u001b[36m(train_lora pid=22919)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/sarapsg/IMDb_LoRA\n",
            "\u001b[36m(train_lora pid=22919)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/sarapsg/IMDb_LoRA\n",
            " 13%|█▎        | 406/3125 [04:58<34:03,  1.33it/s]\n",
            " 13%|█▎        | 406/3125 [04:58<34:03,  1.33it/s]\n",
            " 13%|█▎        | 406/3125 [04:58<34:03,  1.33it/s]\n",
            " 13%|█▎        | 406/3125 [04:58<34:03,  1.33it/s]\n",
            " 13%|█▎        | 406/3125 [04:58<34:03,  1.33it/s]\n",
            " 13%|█▎        | 406/3125 [04:58<34:03,  1.33it/s]\n",
            " 13%|█▎        | 406/3125 [04:58<34:03,  1.33it/s]\n",
            " 13%|█▎        | 406/3125 [04:58<34:03,  1.33it/s]\n",
            " 13%|█▎        | 406/3125 [04:58<34:03,  1.33it/s]\n",
            " 13%|█▎        | 406/3125 [04:58<34:03,  1.33it/s]\n",
            " 13%|█▎        | 406/3125 [04:58<34:03,  1.33it/s]\n",
            " 13%|█▎        | 406/3125 [04:58<34:03,  1.33it/s]\n",
            " 13%|█▎        | 406/3125 [04:58<34:03,  1.33it/s]\n",
            " 13%|█▎        | 406/3125 [04:58<34:03,  1.33it/s]\n",
            " 13%|█▎        | 406/3125 [04:58<34:03,  1.33it/s]\n",
            " 13%|█▎        | 406/3125 [04:58<34:03,  1.33it/s]\n",
            " 13%|█▎        | 406/3125 [04:58<34:03,  1.33it/s]\n",
            " 13%|█▎        | 406/3125 [04:58<34:03,  1.33it/s]\n",
            " 13%|█▎        | 406/3125 [04:58<34:03,  1.33it/s]\n",
            " 13%|█▎        | 406/3125 [04:58<34:03,  1.33it/s]\n",
            " 13%|█▎        | 406/3125 [04:58<34:03,  1.33it/s]\n",
            " 13%|█▎        | 406/3125 [04:58<34:03,  1.33it/s]\n",
            " 13%|█▎        | 407/3125 [04:59<33:46,  1.34it/s]\n",
            " 13%|█▎        | 407/3125 [04:59<33:46,  1.34it/s]\n",
            " 13%|█▎        | 407/3125 [04:59<33:46,  1.34it/s]\n",
            " 13%|█▎        | 407/3125 [04:59<33:46,  1.34it/s]\n",
            " 13%|█▎        | 407/3125 [04:59<33:46,  1.34it/s]\n",
            " 13%|█▎        | 407/3125 [04:59<33:46,  1.34it/s]\n",
            " 13%|█▎        | 407/3125 [04:59<33:46,  1.34it/s]\n",
            " 13%|█▎        | 407/3125 [04:59<33:46,  1.34it/s]\n",
            " 13%|█▎        | 407/3125 [04:59<33:46,  1.34it/s]\n",
            " 13%|█▎        | 407/3125 [04:59<33:46,  1.34it/s]\n",
            " 13%|█▎        | 407/3125 [04:59<33:46,  1.34it/s]\n",
            " 13%|█▎        | 407/3125 [04:59<33:46,  1.34it/s]\n",
            " 13%|█▎        | 407/3125 [04:59<33:46,  1.34it/s]\n",
            " 13%|█▎        | 407/3125 [04:59<33:46,  1.34it/s]\n",
            " 13%|█▎        | 407/3125 [04:59<33:46,  1.34it/s]\n",
            " 13%|█▎        | 407/3125 [04:59<33:46,  1.34it/s]\n",
            " 13%|█▎        | 407/3125 [04:59<33:46,  1.34it/s]\n",
            " 13%|█▎        | 407/3125 [04:59<33:46,  1.34it/s]\n",
            " 13%|█▎        | 407/3125 [04:59<33:46,  1.34it/s]\n",
            " 13%|█▎        | 407/3125 [04:59<33:46,  1.34it/s]\n",
            " 13%|█▎        | 407/3125 [04:59<33:46,  1.34it/s]\n",
            " 13%|█▎        | 407/3125 [04:59<33:46,  1.34it/s]\n",
            " 13%|█▎        | 408/3125 [04:59<33:31,  1.35it/s]\n",
            " 13%|█▎        | 408/3125 [04:59<33:31,  1.35it/s]\n",
            " 13%|█▎        | 408/3125 [04:59<33:31,  1.35it/s]\n",
            " 13%|█▎        | 408/3125 [04:59<33:31,  1.35it/s]\n",
            " 13%|█▎        | 408/3125 [04:59<33:31,  1.35it/s]\n",
            " 13%|█▎        | 408/3125 [04:59<33:31,  1.35it/s]\n",
            " 13%|█▎        | 408/3125 [04:59<33:31,  1.35it/s]\n",
            " 13%|█▎        | 408/3125 [04:59<33:31,  1.35it/s]\n",
            " 13%|█▎        | 408/3125 [04:59<33:31,  1.35it/s]\n",
            " 13%|█▎        | 408/3125 [04:59<33:31,  1.35it/s]\n",
            " 13%|█▎        | 408/3125 [04:59<33:31,  1.35it/s]\n",
            " 13%|█▎        | 408/3125 [04:59<33:31,  1.35it/s]\n",
            " 13%|█▎        | 408/3125 [04:59<33:31,  1.35it/s]\n",
            " 13%|█▎        | 408/3125 [04:59<33:31,  1.35it/s]\n",
            " 13%|█▎        | 408/3125 [04:59<33:31,  1.35it/s]\n",
            " 13%|█▎        | 408/3125 [04:59<33:31,  1.35it/s]\n",
            " 13%|█▎        | 408/3125 [04:59<33:31,  1.35it/s]\n",
            " 13%|█▎        | 408/3125 [04:59<33:31,  1.35it/s]\n",
            " 13%|█▎        | 408/3125 [04:59<33:31,  1.35it/s]\n",
            " 13%|█▎        | 408/3125 [04:59<33:31,  1.35it/s]\n",
            " 13%|█▎        | 408/3125 [04:59<33:31,  1.35it/s]\n",
            " 13%|█▎        | 408/3125 [04:59<33:31,  1.35it/s]\n",
            " 13%|█▎        | 409/3125 [05:00<33:18,  1.36it/s]\n",
            " 13%|█▎        | 409/3125 [05:00<33:18,  1.36it/s]\n",
            " 13%|█▎        | 409/3125 [05:00<33:18,  1.36it/s]\n",
            " 13%|█▎        | 409/3125 [05:00<33:18,  1.36it/s]\n",
            " 13%|█▎        | 409/3125 [05:00<33:18,  1.36it/s]\n",
            " 13%|█▎        | 409/3125 [05:00<33:18,  1.36it/s]\n",
            " 13%|█▎        | 409/3125 [05:00<33:18,  1.36it/s]\n",
            " 13%|█▎        | 409/3125 [05:00<33:18,  1.36it/s]\n",
            " 13%|█▎        | 409/3125 [05:00<33:18,  1.36it/s]\n",
            " 13%|█▎        | 409/3125 [05:00<33:18,  1.36it/s]\n",
            " 13%|█▎        | 409/3125 [05:00<33:18,  1.36it/s]\n",
            " 13%|█▎        | 409/3125 [05:00<33:18,  1.36it/s]\n",
            " 13%|█▎        | 409/3125 [05:00<33:18,  1.36it/s]\n",
            " 13%|█▎        | 409/3125 [05:00<33:18,  1.36it/s]\n",
            " 13%|█▎        | 409/3125 [05:00<33:18,  1.36it/s]\n",
            " 13%|█▎        | 409/3125 [05:00<33:18,  1.36it/s]\n",
            " 13%|█▎        | 409/3125 [05:00<33:18,  1.36it/s]\n",
            " 13%|█▎        | 409/3125 [05:00<33:18,  1.36it/s]\n",
            "Map:   0%|          | 0/25000 [00:00<?, ? examples/s]\n",
            " 13%|█▎        | 409/3125 [05:00<33:18,  1.36it/s]\n",
            "Map:   0%|          | 0/25000 [00:00<?, ? examples/s]\n",
            " 13%|█▎        | 409/3125 [05:00<33:18,  1.36it/s]\n",
            "Map:   0%|          | 0/25000 [00:00<?, ? examples/s]\n",
            " 13%|█▎        | 409/3125 [05:00<33:18,  1.36it/s]\n",
            "Map:   0%|          | 0/25000 [00:00<?, ? examples/s]\n",
            " 13%|█▎        | 409/3125 [05:00<33:18,  1.36it/s]\n",
            "Map:   0%|          | 0/25000 [00:00<?, ? examples/s]\n",
            "Map:   0%|          | 0/25000 [00:00<?, ? examples/s]\n",
            "Map:   0%|          | 0/25000 [00:00<?, ? examples/s]\n",
            "Map:   0%|          | 0/25000 [00:00<?, ? examples/s]\n",
            "Map:   0%|          | 0/25000 [00:00<?, ? examples/s]\n",
            "Map:   0%|          | 0/25000 [00:00<?, ? examples/s]\n",
            "Map:   0%|          | 0/25000 [00:00<?, ? examples/s]\n",
            "Map:   0%|          | 0/25000 [00:00<?, ? examples/s]\n",
            "Map:   0%|          | 0/25000 [00:00<?, ? examples/s]\n",
            "Map:   0%|          | 0/25000 [00:00<?, ? examples/s]\n",
            "Map:   0%|          | 0/25000 [00:00<?, ? examples/s]\n",
            "Map:   0%|          | 0/25000 [00:00<?, ? examples/s]\n",
            "Map:   0%|          | 0/25000 [00:00<?, ? examples/s]\n",
            "Map:   0%|          | 0/25000 [00:00<?, ? examples/s]\n",
            "Map:   0%|          | 0/25000 [00:00<?, ? examples/s]\n",
            "Map:   0%|          | 0/25000 [00:00<?, ? examples/s]\n",
            "Map:   0%|          | 0/25000 [00:00<?, ? examples/s]\n",
            "Map:   0%|          | 0/25000 [00:00<?, ? examples/s]\n",
            "Map:   8%|▊         | 1939/25000 [00:00<00:01, 18003.50 examples/s]\n",
            "Map:   8%|▊         | 1939/25000 [00:00<00:01, 18003.50 examples/s]\n",
            "Map:   8%|▊         | 1939/25000 [00:00<00:01, 18003.50 examples/s]\n",
            "Map:   8%|▊         | 1939/25000 [00:00<00:01, 18003.50 examples/s]\n",
            "Map:   8%|▊         | 1939/25000 [00:00<00:01, 18003.50 examples/s]\n",
            "Map:   8%|▊         | 1939/25000 [00:00<00:01, 18003.50 examples/s]\n",
            "Map:   8%|▊         | 1939/25000 [00:00<00:01, 18003.50 examples/s]\n",
            "Map:   8%|▊         | 1939/25000 [00:00<00:01, 18003.50 examples/s]\n",
            "Map:   8%|▊         | 1939/25000 [00:00<00:01, 18003.50 examples/s]\n",
            "Map:   8%|▊         | 1939/25000 [00:00<00:01, 18003.50 examples/s]\n",
            "Map:   8%|▊         | 1939/25000 [00:00<00:01, 18003.50 examples/s]\n",
            "Map:   8%|▊         | 1939/25000 [00:00<00:01, 18003.50 examples/s]\n",
            "Map:   8%|▊         | 1939/25000 [00:00<00:01, 18003.50 examples/s]\n",
            "Map:   8%|▊         | 1939/25000 [00:00<00:01, 18003.50 examples/s]\n",
            "Map:   8%|▊         | 1939/25000 [00:00<00:01, 18003.50 examples/s]\n",
            "Map:   8%|▊         | 1939/25000 [00:00<00:01, 18003.50 examples/s]\n",
            "Map:   8%|▊         | 1939/25000 [00:00<00:01, 18003.50 examples/s]\n",
            "Map:   8%|▊         | 1939/25000 [00:00<00:01, 18003.50 examples/s]\n",
            "Map:   8%|▊         | 1939/25000 [00:00<00:01, 18003.50 examples/s]\n",
            "Map:   8%|▊         | 1939/25000 [00:00<00:01, 18003.50 examples/s]\n",
            "Map:   8%|▊         | 1939/25000 [00:00<00:01, 18003.50 examples/s]\n",
            "Map:   8%|▊         | 1939/25000 [00:00<00:01, 18003.50 examples/s]\n",
            "Map:  19%|█▉        | 4702/25000 [00:00<00:01, 18278.01 examples/s]\n",
            "Map:  19%|█▉        | 4702/25000 [00:00<00:01, 18278.01 examples/s]\n",
            "Map:  19%|█▉        | 4702/25000 [00:00<00:01, 18278.01 examples/s]\n",
            "Map:  19%|█▉        | 4702/25000 [00:00<00:01, 18278.01 examples/s]\n",
            "Map:  19%|█▉        | 4702/25000 [00:00<00:01, 18278.01 examples/s]\n",
            "Map:  19%|█▉        | 4702/25000 [00:00<00:01, 18278.01 examples/s]\n",
            "Map:  19%|█▉        | 4702/25000 [00:00<00:01, 18278.01 examples/s]\n",
            "Map:  19%|█▉        | 4702/25000 [00:00<00:01, 18278.01 examples/s]\n",
            "Map:  19%|█▉        | 4702/25000 [00:00<00:01, 18278.01 examples/s]\n",
            "Map:  19%|█▉        | 4702/25000 [00:00<00:01, 18278.01 examples/s]\n",
            "Map:  19%|█▉        | 4702/25000 [00:00<00:01, 18278.01 examples/s]\n",
            "Map:  19%|█▉        | 4702/25000 [00:00<00:01, 18278.01 examples/s]\n",
            "Map:  19%|█▉        | 4702/25000 [00:00<00:01, 18278.01 examples/s]\n",
            "Map:  19%|█▉        | 4702/25000 [00:00<00:01, 18278.01 examples/s]\n",
            "Map:  19%|█▉        | 4702/25000 [00:00<00:01, 18278.01 examples/s]\n",
            "Map:  19%|█▉        | 4702/25000 [00:00<00:01, 18278.01 examples/s]\n",
            "Map:  19%|█▉        | 4702/25000 [00:00<00:01, 18278.01 examples/s]\n",
            "Map:  19%|█▉        | 4702/25000 [00:00<00:01, 18278.01 examples/s]\n",
            "Map:  19%|█▉        | 4702/25000 [00:00<00:01, 18278.01 examples/s]\n",
            "Map:  19%|█▉        | 4702/25000 [00:00<00:01, 18278.01 examples/s]\n",
            "Map:  19%|█▉        | 4702/25000 [00:00<00:01, 18278.01 examples/s]\n",
            "Map:  19%|█▉        | 4702/25000 [00:00<00:01, 18278.01 examples/s]\n",
            "Map:  26%|██▌       | 6536/25000 [00:00<00:01, 18298.95 examples/s]\n",
            "Map:  26%|██▌       | 6536/25000 [00:00<00:01, 18298.95 examples/s]\n",
            "Map:  26%|██▌       | 6536/25000 [00:00<00:01, 18298.95 examples/s]\n",
            "Map:  26%|██▌       | 6536/25000 [00:00<00:01, 18298.95 examples/s]\n",
            "Map:  26%|██▌       | 6536/25000 [00:00<00:01, 18298.95 examples/s]\n",
            "Map:  26%|██▌       | 6536/25000 [00:00<00:01, 18298.95 examples/s]\n",
            "Map:  26%|██▌       | 6536/25000 [00:00<00:01, 18298.95 examples/s]\n",
            "Map:  26%|██▌       | 6536/25000 [00:00<00:01, 18298.95 examples/s]\n",
            "Map:  26%|██▌       | 6536/25000 [00:00<00:01, 18298.95 examples/s]\n",
            "Map:  26%|██▌       | 6536/25000 [00:00<00:01, 18298.95 examples/s]\n",
            "Map:  26%|██▌       | 6536/25000 [00:00<00:01, 18298.95 examples/s]\n",
            "Map:  26%|██▌       | 6536/25000 [00:00<00:01, 18298.95 examples/s]\n",
            "Map:  26%|██▌       | 6536/25000 [00:00<00:01, 18298.95 examples/s]\n",
            "Map:  26%|██▌       | 6536/25000 [00:00<00:01, 18298.95 examples/s]\n",
            "Map:  26%|██▌       | 6536/25000 [00:00<00:01, 18298.95 examples/s]\n",
            "Map:  26%|██▌       | 6536/25000 [00:00<00:01, 18298.95 examples/s]\n",
            "Map:  26%|██▌       | 6536/25000 [00:00<00:01, 18298.95 examples/s]\n",
            "Map:  26%|██▌       | 6536/25000 [00:00<00:01, 18298.95 examples/s]\n",
            "Map:  26%|██▌       | 6536/25000 [00:00<00:01, 18298.95 examples/s]\n",
            "Map:  26%|██▌       | 6536/25000 [00:00<00:01, 18298.95 examples/s]\n",
            "Map:  26%|██▌       | 6536/25000 [00:00<00:01, 18298.95 examples/s]\n",
            "Map:  26%|██▌       | 6536/25000 [00:00<00:01, 18298.95 examples/s]\n",
            "Map:  34%|███▎      | 8432/25000 [00:00<00:00, 18532.79 examples/s]\n",
            "Map:  34%|███▎      | 8432/25000 [00:00<00:00, 18532.79 examples/s]\n",
            "Map:  34%|███▎      | 8432/25000 [00:00<00:00, 18532.79 examples/s]\n",
            "Map:  34%|███▎      | 8432/25000 [00:00<00:00, 18532.79 examples/s]\n",
            "Map:  34%|███▎      | 8432/25000 [00:00<00:00, 18532.79 examples/s]\n",
            "Map:  34%|███▎      | 8432/25000 [00:00<00:00, 18532.79 examples/s]\n",
            "Map:  34%|███▎      | 8432/25000 [00:00<00:00, 18532.79 examples/s]\n",
            "Map:  34%|███▎      | 8432/25000 [00:00<00:00, 18532.79 examples/s]\n",
            "Map:  34%|███▎      | 8432/25000 [00:00<00:00, 18532.79 examples/s]\n",
            "Map:  34%|███▎      | 8432/25000 [00:00<00:00, 18532.79 examples/s]\n",
            "Map:  34%|███▎      | 8432/25000 [00:00<00:00, 18532.79 examples/s]\n",
            "Map:  34%|███▎      | 8432/25000 [00:00<00:00, 18532.79 examples/s]\n",
            "Map:  34%|███▎      | 8432/25000 [00:00<00:00, 18532.79 examples/s]\n",
            "Map:  34%|███▎      | 8432/25000 [00:00<00:00, 18532.79 examples/s]\n",
            "Map:  34%|███▎      | 8432/25000 [00:00<00:00, 18532.79 examples/s]\n",
            "Map:  34%|███▎      | 8432/25000 [00:00<00:00, 18532.79 examples/s]\n",
            "Map:  34%|███▎      | 8432/25000 [00:00<00:00, 18532.79 examples/s]\n",
            "Map:  34%|███▎      | 8432/25000 [00:00<00:00, 18532.79 examples/s]\n",
            "Map:  34%|███▎      | 8432/25000 [00:00<00:00, 18532.79 examples/s]\n",
            "Map:  34%|███▎      | 8432/25000 [00:00<00:00, 18532.79 examples/s]\n",
            "Map:  34%|███▎      | 8432/25000 [00:00<00:00, 18532.79 examples/s]\n",
            "Map:  34%|███▎      | 8432/25000 [00:00<00:00, 18532.79 examples/s]\n",
            "Map:  44%|████▍     | 11000/25000 [00:00<00:00, 17901.88 examples/s]\n",
            "Map:  44%|████▍     | 11000/25000 [00:00<00:00, 17901.88 examples/s]\n",
            "Map:  44%|████▍     | 11000/25000 [00:00<00:00, 17901.88 examples/s]\n",
            "Map:  44%|████▍     | 11000/25000 [00:00<00:00, 17901.88 examples/s]\n",
            "Map:  44%|████▍     | 11000/25000 [00:00<00:00, 17901.88 examples/s]\n",
            "Map:  44%|████▍     | 11000/25000 [00:00<00:00, 17901.88 examples/s]\n",
            "Map:  44%|████▍     | 11000/25000 [00:00<00:00, 17901.88 examples/s]\n",
            "Map:  44%|████▍     | 11000/25000 [00:00<00:00, 17901.88 examples/s]\n",
            " 13%|█▎        | 410/3125 [05:01<33:33,  1.35it/s]\n",
            " 13%|█▎        | 410/3125 [05:01<33:33,  1.35it/s]\n",
            " 13%|█▎        | 410/3125 [05:01<33:33,  1.35it/s]\n",
            " 13%|█▎        | 410/3125 [05:01<33:33,  1.35it/s]\n",
            " 13%|█▎        | 410/3125 [05:01<33:33,  1.35it/s]\n",
            " 13%|█▎        | 410/3125 [05:01<33:33,  1.35it/s]\n",
            " 13%|█▎        | 410/3125 [05:01<33:33,  1.35it/s]\n",
            " 13%|█▎        | 410/3125 [05:01<33:33,  1.35it/s]\n",
            " 13%|█▎        | 410/3125 [05:01<33:33,  1.35it/s]\n",
            " 13%|█▎        | 410/3125 [05:01<33:33,  1.35it/s]\n",
            " 13%|█▎        | 410/3125 [05:01<33:33,  1.35it/s]\n",
            " 13%|█▎        | 410/3125 [05:01<33:33,  1.35it/s]\n",
            "Map:  13%|█▎        | 3175/25000 [00:00<00:01, 15909.28 examples/s]\n",
            " 13%|█▎        | 410/3125 [05:01<33:33,  1.35it/s]\n",
            "Map:  13%|█▎        | 3175/25000 [00:00<00:01, 15909.28 examples/s]\n",
            " 13%|█▎        | 410/3125 [05:01<33:33,  1.35it/s]\n",
            " 13%|█▎        | 410/3125 [05:01<33:33,  1.35it/s]\n",
            " 13%|█▎        | 410/3125 [05:01<33:33,  1.35it/s]\n",
            " 13%|█▎        | 410/3125 [05:01<33:33,  1.35it/s]\n",
            " 13%|█▎        | 410/3125 [05:01<33:33,  1.35it/s]\n",
            " 13%|█▎        | 410/3125 [05:01<33:33,  1.35it/s]\n",
            " 13%|█▎        | 410/3125 [05:01<33:33,  1.35it/s]\n",
            " 13%|█▎        | 410/3125 [05:01<33:33,  1.35it/s]\n",
            " 13%|█▎        | 410/3125 [05:01<33:33,  1.35it/s]\n",
            "Map:  96%|█████████▌| 23983/25000 [00:01<00:00, 17426.73 examples/s]\n",
            "Map:  96%|█████████▌| 23983/25000 [00:01<00:00, 17426.73 examples/s]\n",
            "Map:  96%|█████████▌| 23983/25000 [00:01<00:00, 17426.73 examples/s]\n",
            "Map:  96%|█████████▌| 23983/25000 [00:01<00:00, 17426.73 examples/s]\n",
            "Map:  96%|█████████▌| 23983/25000 [00:01<00:00, 17426.73 examples/s]\n",
            "Map:  96%|█████████▌| 23983/25000 [00:01<00:00, 17426.73 examples/s]\n",
            "Map:  96%|█████████▌| 23983/25000 [00:01<00:00, 17426.73 examples/s]\n",
            "Map:  96%|█████████▌| 23983/25000 [00:01<00:00, 17426.73 examples/s]\n",
            "Map:  96%|█████████▌| 23983/25000 [00:01<00:00, 17426.73 examples/s]\n",
            "Map:  96%|█████████▌| 23983/25000 [00:01<00:00, 17426.73 examples/s]\n",
            "Map:  96%|█████████▌| 23983/25000 [00:01<00:00, 17426.73 examples/s]\n",
            "Map:  96%|█████████▌| 23983/25000 [00:01<00:00, 17426.73 examples/s]\n",
            "Map:  96%|█████████▌| 23983/25000 [00:01<00:00, 17426.73 examples/s]\n",
            "Map:  96%|█████████▌| 23983/25000 [00:01<00:00, 17426.73 examples/s]\n",
            "Map:  96%|█████████▌| 23983/25000 [00:01<00:00, 17426.73 examples/s]\n",
            "Map:  96%|█████████▌| 23983/25000 [00:01<00:00, 17426.73 examples/s]\n",
            "Map:  96%|█████████▌| 23983/25000 [00:01<00:00, 17426.73 examples/s]\n",
            "Map:  96%|█████████▌| 23983/25000 [00:01<00:00, 17426.73 examples/s]\n",
            "Map:  96%|█████████▌| 23983/25000 [00:01<00:00, 17426.73 examples/s]\n",
            "Map:  96%|█████████▌| 23983/25000 [00:01<00:00, 17426.73 examples/s]\n",
            "Map: 100%|██████████| 25000/25000 [00:01<00:00, 17407.26 examples/s]\n",
            "Map: 100%|██████████| 25000/25000 [00:01<00:00, 17407.26 examples/s]\n",
            "Map: 100%|██████████| 25000/25000 [00:01<00:00, 17407.26 examples/s]\n",
            "Map: 100%|██████████| 25000/25000 [00:01<00:00, 17407.26 examples/s]\n",
            "Map: 100%|██████████| 25000/25000 [00:01<00:00, 17407.26 examples/s]\n",
            " 13%|█▎        | 411/3125 [05:02<34:11,  1.32it/s]\n",
            "Map: 100%|██████████| 25000/25000 [00:01<00:00, 17407.26 examples/s]\n",
            " 13%|█▎        | 411/3125 [05:02<34:11,  1.32it/s]\n",
            " 13%|█▎        | 411/3125 [05:02<34:11,  1.32it/s]\n",
            " 13%|█▎        | 411/3125 [05:02<34:11,  1.32it/s]\n",
            " 13%|█▎        | 411/3125 [05:02<34:11,  1.32it/s]\n",
            "Map: 100%|██████████| 25000/25000 [00:01<00:00, 17407.26 examples/s]\n",
            " 13%|█▎        | 411/3125 [05:02<34:11,  1.32it/s]\n",
            " 13%|█▎        | 411/3125 [05:02<34:11,  1.32it/s]\n",
            " 13%|█▎        | 411/3125 [05:02<34:11,  1.32it/s]\n",
            " 13%|█▎        | 411/3125 [05:02<34:11,  1.32it/s]\n",
            " 13%|█▎        | 411/3125 [05:02<34:11,  1.32it/s]\n",
            " 13%|█▎        | 411/3125 [05:02<34:11,  1.32it/s]\n",
            " 13%|█▎        | 411/3125 [05:02<34:11,  1.32it/s]\n",
            "Map: 100%|██████████| 25000/25000 [00:01<00:00, 17407.26 examples/s]\n",
            "Map:   3%|▎         | 863/25000 [00:00<00:05, 4267.48 examples/s]\n",
            " 13%|█▎        | 411/3125 [05:02<34:11,  1.32it/s]\n",
            "Map: 100%|██████████| 25000/25000 [00:01<00:00, 17407.26 examples/s]\n",
            "Map:   3%|▎         | 863/25000 [00:00<00:05, 4267.48 examples/s]\n",
            "Map: 100%|██████████| 25000/25000 [00:01<00:00, 17407.26 examples/s]\n",
            "Map:   3%|▎         | 863/25000 [00:00<00:05, 4267.48 examples/s]\n",
            "Map: 100%|██████████| 25000/25000 [00:01<00:00, 17407.26 examples/s]\n",
            "Map:   3%|▎         | 863/25000 [00:00<00:05, 4267.48 examples/s]\n",
            " 13%|█▎        | 411/3125 [05:02<34:11,  1.32it/s]\n",
            "Map: 100%|██████████| 25000/25000 [00:01<00:00, 17407.26 examples/s]\n",
            "Map:   3%|▎         | 863/25000 [00:00<00:05, 4267.48 examples/s]\n",
            "Map: 100%|██████████| 25000/25000 [00:01<00:00, 17407.26 examples/s]\n",
            "Map:   3%|▎         | 863/25000 [00:00<00:05, 4267.48 examples/s]\n",
            " 13%|█▎        | 411/3125 [05:02<34:11,  1.32it/s]\n",
            "Map: 100%|██████████| 25000/25000 [00:01<00:00, 17407.26 examples/s]\n",
            "Map:   3%|▎         | 863/25000 [00:00<00:05, 4267.48 examples/s]\n",
            "Map: 100%|██████████| 25000/25000 [00:01<00:00, 17407.26 examples/s]\n",
            "Map:   3%|▎         | 863/25000 [00:00<00:05, 4267.48 examples/s]\n",
            " 13%|█▎        | 411/3125 [05:02<34:11,  1.32it/s]\n",
            " 13%|█▎        | 411/3125 [05:02<34:11,  1.32it/s]\n",
            "Map: 100%|██████████| 25000/25000 [00:01<00:00, 17407.26 examples/s]\n",
            "Map:   3%|▎         | 863/25000 [00:00<00:05, 4267.48 examples/s]\n",
            " 13%|█▎        | 411/3125 [05:02<34:11,  1.32it/s]\n",
            "Map: 100%|██████████| 25000/25000 [00:01<00:00, 17407.26 examples/s]\n",
            "Map:   3%|▎         | 863/25000 [00:00<00:05, 4267.48 examples/s]\n",
            "Map: 100%|██████████| 25000/25000 [00:01<00:00, 17407.26 examples/s]\n",
            "Map:   3%|▎         | 863/25000 [00:00<00:05, 4267.48 examples/s]\n",
            " 13%|█▎        | 411/3125 [05:02<34:11,  1.32it/s]\n",
            " 13%|█▎        | 411/3125 [05:02<34:11,  1.32it/s]\n",
            "Map: 100%|██████████| 25000/25000 [00:01<00:00, 17407.26 examples/s]\n",
            "Map:   3%|▎         | 863/25000 [00:00<00:05, 4267.48 examples/s]\n",
            "Map: 100%|██████████| 25000/25000 [00:01<00:00, 17407.26 examples/s]\n",
            "Map:   3%|▎         | 863/25000 [00:00<00:05, 4267.48 examples/s]\n",
            " 13%|█▎        | 411/3125 [05:02<34:11,  1.32it/s]\n",
            "Map: 100%|██████████| 25000/25000 [00:01<00:00, 17407.26 examples/s]\n",
            "Map:   3%|▎         | 863/25000 [00:00<00:05, 4267.48 examples/s]\n",
            " 13%|█▎        | 411/3125 [05:02<34:11,  1.32it/s]\n",
            "Map: 100%|██████████| 25000/25000 [00:01<00:00, 17407.26 examples/s]\n",
            "Map:   3%|▎         | 863/25000 [00:00<00:05, 4267.48 examples/s]\n",
            "\u001b[36m(train_lora pid=22917)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/sarapsg/IMDb_LoRA\u001b[32m [repeated 30x across cluster]\u001b[0m\n",
            " 13%|█▎        | 412/3125 [05:03<37:24,  1.21it/s]\n",
            " 13%|█▎        | 412/3125 [05:03<37:24,  1.21it/s]\n",
            " 13%|█▎        | 412/3125 [05:03<37:24,  1.21it/s]\n",
            " 13%|█▎        | 412/3125 [05:03<37:24,  1.21it/s]\n",
            " 13%|█▎        | 412/3125 [05:03<37:24,  1.21it/s]\n",
            " 13%|█▎        | 412/3125 [05:03<37:24,  1.21it/s]\n",
            " 13%|█▎        | 412/3125 [05:03<37:24,  1.21it/s]\n",
            "\u001b[36m(train_lora pid=22917)\u001b[0m wandb: Tracking run with wandb version 0.19.8\u001b[32m [repeated 33x across cluster]\u001b[0m\n",
            "\u001b[36m(train_lora pid=22917)\u001b[0m wandb: Run data is saved locally in /content/wandb/run-20250320_043312-i64q4yvd\u001b[32m [repeated 33x across cluster]\u001b[0m\n",
            "\u001b[36m(train_lora pid=22917)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\u001b[32m [repeated 33x across cluster]\u001b[0m\n",
            "\u001b[36m(train_lora pid=22917)\u001b[0m wandb: Syncing run dark-monkey-3\u001b[32m [repeated 33x across cluster]\u001b[0m\n",
            "\u001b[36m(train_lora pid=22917)\u001b[0m wandb: 🚀 View run at https://wandb.ai/sarapsg/IMDb_LoRA/runs/i64q4yvd\u001b[32m [repeated 33x across cluster]\u001b[0m\n",
            " 13%|█▎        | 412/3125 [05:03<37:24,  1.21it/s]\n",
            " 13%|█▎        | 412/3125 [05:03<37:24,  1.21it/s]\n",
            " 13%|█▎        | 412/3125 [05:03<37:24,  1.21it/s]\n",
            " 13%|█▎        | 412/3125 [05:03<37:24,  1.21it/s]\n",
            " 13%|█▎        | 412/3125 [05:03<37:24,  1.21it/s]\n",
            " 13%|█▎        | 412/3125 [05:03<37:24,  1.21it/s]\n",
            " 13%|█▎        | 412/3125 [05:03<37:24,  1.21it/s]\n",
            " 13%|█▎        | 412/3125 [05:03<37:24,  1.21it/s]\n",
            " 13%|█▎        | 412/3125 [05:03<37:24,  1.21it/s]\n",
            " 13%|█▎        | 412/3125 [05:03<37:24,  1.21it/s]\n",
            " 13%|█▎        | 412/3125 [05:03<37:24,  1.21it/s]\n",
            " 13%|█▎        | 412/3125 [05:03<37:24,  1.21it/s]\n",
            " 13%|█▎        | 412/3125 [05:03<37:24,  1.21it/s]\n",
            " 13%|█▎        | 412/3125 [05:03<37:24,  1.21it/s]\n",
            " 13%|█▎        | 412/3125 [05:03<37:24,  1.21it/s]\n",
            "Map: 100%|██████████| 25000/25000 [00:01<00:00, 14616.96 examples/s]\n",
            "Map: 100%|██████████| 25000/25000 [00:01<00:00, 14616.96 examples/s]\n",
            "Map: 100%|██████████| 25000/25000 [00:01<00:00, 14616.96 examples/s]\n",
            "Map: 100%|██████████| 25000/25000 [00:01<00:00, 14616.96 examples/s]\n",
            "Map: 100%|██████████| 25000/25000 [00:01<00:00, 14616.96 examples/s]\n",
            "Map: 100%|██████████| 25000/25000 [00:01<00:00, 14616.96 examples/s]\n",
            "Map: 100%|██████████| 25000/25000 [00:01<00:00, 14616.96 examples/s]\n",
            "Map: 100%|██████████| 25000/25000 [00:01<00:00, 14616.96 examples/s]\n",
            "Map: 100%|██████████| 25000/25000 [00:01<00:00, 14616.96 examples/s]\n",
            "Map: 100%|██████████| 25000/25000 [00:01<00:00, 14616.96 examples/s]\n",
            "Map: 100%|██████████| 25000/25000 [00:01<00:00, 14616.96 examples/s]\n",
            " 13%|█▎        | 413/3125 [05:04<37:38,  1.20it/s]\n",
            " 13%|█▎        | 413/3125 [05:04<37:38,  1.20it/s]\n",
            " 13%|█▎        | 413/3125 [05:04<37:38,  1.20it/s]\n",
            " 13%|█▎        | 413/3125 [05:04<37:38,  1.20it/s]\n",
            " 13%|█▎        | 413/3125 [05:04<37:38,  1.20it/s]\n",
            " 13%|█▎        | 413/3125 [05:04<37:38,  1.20it/s]\n",
            " 13%|█▎        | 413/3125 [05:04<37:38,  1.20it/s]\n",
            " 13%|█▎        | 413/3125 [05:04<37:38,  1.20it/s]\n",
            " 13%|█▎        | 413/3125 [05:04<37:38,  1.20it/s]\n",
            " 13%|█▎        | 413/3125 [05:04<37:38,  1.20it/s]\n",
            " 13%|█▎        | 413/3125 [05:04<37:38,  1.20it/s]\n",
            " 13%|█▎        | 413/3125 [05:04<37:38,  1.20it/s]\n",
            " 13%|█▎        | 413/3125 [05:04<37:38,  1.20it/s]\n",
            " 13%|█▎        | 413/3125 [05:04<37:38,  1.20it/s]\n",
            " 13%|█▎        | 413/3125 [05:04<37:38,  1.20it/s]\n",
            " 13%|█▎        | 413/3125 [05:04<37:38,  1.20it/s]\n",
            " 13%|█▎        | 413/3125 [05:04<37:38,  1.20it/s]\n",
            " 13%|█▎        | 413/3125 [05:04<37:38,  1.20it/s]\n",
            " 13%|█▎        | 413/3125 [05:04<37:38,  1.20it/s]\n",
            " 13%|█▎        | 413/3125 [05:04<37:38,  1.20it/s]\n",
            " 13%|█▎        | 413/3125 [05:04<37:38,  1.20it/s]\n",
            " 13%|█▎        | 413/3125 [05:04<37:38,  1.20it/s]\n",
            " 13%|█▎        | 414/3125 [05:04<37:10,  1.22it/s]\n",
            " 13%|█▎        | 414/3125 [05:04<37:10,  1.22it/s]\n",
            " 13%|█▎        | 414/3125 [05:04<37:10,  1.22it/s]\n",
            " 13%|█▎        | 414/3125 [05:04<37:10,  1.22it/s]\n",
            " 13%|█▎        | 414/3125 [05:04<37:10,  1.22it/s]\n",
            " 13%|█▎        | 414/3125 [05:04<37:10,  1.22it/s]\n",
            " 13%|█▎        | 414/3125 [05:04<37:10,  1.22it/s]\n",
            " 13%|█▎        | 414/3125 [05:04<37:10,  1.22it/s]\n",
            " 13%|█▎        | 414/3125 [05:04<37:10,  1.22it/s]\n",
            " 13%|█▎        | 414/3125 [05:04<37:10,  1.22it/s]\n",
            " 13%|█▎        | 414/3125 [05:04<37:10,  1.22it/s]\n",
            " 13%|█▎        | 414/3125 [05:04<37:10,  1.22it/s]\n",
            " 13%|█▎        | 414/3125 [05:04<37:10,  1.22it/s]\n",
            " 13%|█▎        | 414/3125 [05:04<37:10,  1.22it/s]\n",
            " 13%|█▎        | 414/3125 [05:04<37:10,  1.22it/s]\n",
            " 13%|█▎        | 414/3125 [05:04<37:10,  1.22it/s]\n",
            " 13%|█▎        | 414/3125 [05:04<37:10,  1.22it/s]\n",
            " 13%|█▎        | 414/3125 [05:04<37:10,  1.22it/s]\n",
            " 13%|█▎        | 414/3125 [05:04<37:10,  1.22it/s]\n",
            " 13%|█▎        | 414/3125 [05:04<37:10,  1.22it/s]\n",
            " 13%|█▎        | 414/3125 [05:04<37:10,  1.22it/s]\n",
            " 13%|█▎        | 414/3125 [05:04<37:10,  1.22it/s]\n",
            "Map:   0%|          | 0/50000 [00:00<?, ? examples/s]\u001b[32m [repeated 94x across cluster]\u001b[0m\n",
            " 13%|█▎        | 415/3125 [05:05<38:24,  1.18it/s]\n",
            " 13%|█▎        | 415/3125 [05:05<38:24,  1.18it/s]\n",
            " 13%|█▎        | 415/3125 [05:05<38:24,  1.18it/s]\n",
            " 13%|█▎        | 415/3125 [05:05<38:24,  1.18it/s]\n",
            " 13%|█▎        | 415/3125 [05:05<38:24,  1.18it/s]\n",
            " 13%|█▎        | 415/3125 [05:05<38:24,  1.18it/s]\n",
            " 13%|█▎        | 415/3125 [05:05<38:24,  1.18it/s]\n",
            " 13%|█▎        | 415/3125 [05:05<38:24,  1.18it/s]\n",
            " 13%|█▎        | 415/3125 [05:05<38:24,  1.18it/s]\n",
            " 13%|█▎        | 415/3125 [05:05<38:24,  1.18it/s]\n",
            " 13%|█▎        | 415/3125 [05:05<38:24,  1.18it/s]\n",
            " 13%|█▎        | 415/3125 [05:05<38:24,  1.18it/s]\n",
            " 13%|█▎        | 415/3125 [05:05<38:24,  1.18it/s]\n",
            " 13%|█▎        | 415/3125 [05:05<38:24,  1.18it/s]\n",
            " 13%|█▎        | 415/3125 [05:05<38:24,  1.18it/s]\n",
            " 13%|█▎        | 415/3125 [05:05<38:24,  1.18it/s]\n",
            " 13%|█▎        | 415/3125 [05:05<38:24,  1.18it/s]\n",
            " 13%|█▎        | 415/3125 [05:05<38:24,  1.18it/s]\n",
            "Map:  33%|███▎      | 16631/50000 [00:01<00:02, 15576.42 examples/s]\u001b[32m [repeated 1364x across cluster]\u001b[0m\n",
            " 13%|█▎        | 415/3125 [05:05<38:24,  1.18it/s]\n",
            " 13%|█▎        | 415/3125 [05:05<38:24,  1.18it/s]\n",
            " 13%|█▎        | 415/3125 [05:05<38:24,  1.18it/s]\n",
            " 13%|█▎        | 415/3125 [05:05<38:24,  1.18it/s]\n",
            "Map:  88%|████████▊ | 43839/50000 [00:02<00:00, 17437.92 examples/s]\u001b[32m [repeated 40x across cluster]\u001b[0m\n",
            " 13%|█▎        | 416/3125 [05:06<37:30,  1.20it/s]\n",
            " 13%|█▎        | 416/3125 [05:06<37:30,  1.20it/s]\n",
            " 13%|█▎        | 416/3125 [05:06<37:30,  1.20it/s]\n",
            " 13%|█▎        | 416/3125 [05:06<37:30,  1.20it/s]\n",
            " 13%|█▎        | 416/3125 [05:06<37:30,  1.20it/s]\n",
            " 13%|█▎        | 416/3125 [05:06<37:30,  1.20it/s]\n",
            " 13%|█▎        | 416/3125 [05:06<37:30,  1.20it/s]\n",
            " 13%|█▎        | 416/3125 [05:06<37:30,  1.20it/s]\n",
            " 13%|█▎        | 416/3125 [05:06<37:30,  1.20it/s]\n",
            " 13%|█▎        | 416/3125 [05:06<37:30,  1.20it/s]\n",
            " 13%|█▎        | 416/3125 [05:06<37:30,  1.20it/s]\n",
            " 13%|█▎        | 416/3125 [05:06<37:30,  1.20it/s]\n",
            " 13%|█▎        | 416/3125 [05:06<37:30,  1.20it/s]\n",
            " 13%|█▎        | 416/3125 [05:06<37:30,  1.20it/s]\n",
            " 13%|█▎        | 416/3125 [05:06<37:30,  1.20it/s]\n",
            " 13%|█▎        | 416/3125 [05:06<37:30,  1.20it/s]\n",
            " 13%|█▎        | 416/3125 [05:06<37:30,  1.20it/s]\n",
            " 13%|█▎        | 416/3125 [05:06<37:30,  1.20it/s]\n",
            " 13%|█▎        | 416/3125 [05:06<37:30,  1.20it/s]\n",
            " 13%|█▎        | 416/3125 [05:06<37:30,  1.20it/s]\n",
            " 13%|█▎        | 416/3125 [05:06<37:30,  1.20it/s]\n",
            " 13%|█▎        | 416/3125 [05:06<37:30,  1.20it/s]\n",
            "\u001b[36m(train_lora pid=22919)\u001b[0m \n",
            "Map: 100%|██████████| 50000/50000 [00:03<00:00, 16571.70 examples/s]\u001b[32m [repeated 156x across cluster]\u001b[0m\n",
            "Map: 100%|██████████| 50000/50000 [00:03<00:00, 16571.70 examples/s]\u001b[32m [repeated 10x across cluster]\u001b[0m\n",
            "\u001b[36m(train_lora pid=22919)\u001b[0m The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
            "\u001b[36m(train_lora pid=22919)\u001b[0m The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
            "\u001b[36m(train_lora pid=22919)\u001b[0m The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
            "\u001b[36m(train_lora pid=22919)\u001b[0m The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
            "\u001b[36m(train_lora pid=22919)\u001b[0m The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
            "\u001b[36m(train_lora pid=22919)\u001b[0m The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
            "\u001b[36m(train_lora pid=22919)\u001b[0m The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
            "\u001b[36m(train_lora pid=22919)\u001b[0m The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
            "\u001b[36m(train_lora pid=22919)\u001b[0m The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
            "\u001b[36m(train_lora pid=22919)\u001b[0m The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
            "\u001b[36m(train_lora pid=22919)\u001b[0m The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
            "\u001b[36m(train_lora pid=22919)\u001b[0m The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
            "\u001b[36m(train_lora pid=22919)\u001b[0m The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
            "\u001b[36m(train_lora pid=22919)\u001b[0m The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
            "\u001b[36m(train_lora pid=22919)\u001b[0m The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
            "\u001b[36m(train_lora pid=22919)\u001b[0m The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
            "\u001b[36m(train_lora pid=22919)\u001b[0m The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
            "\u001b[36m(train_lora pid=22919)\u001b[0m The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
            "\u001b[36m(train_lora pid=22919)\u001b[0m The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
            "\u001b[36m(train_lora pid=22919)\u001b[0m The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
            "\u001b[36m(train_lora pid=22919)\u001b[0m The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
            "\u001b[36m(train_lora pid=22919)\u001b[0m The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
            " 13%|█▎        | 417/3125 [05:07<36:53,  1.22it/s]\n",
            " 13%|█▎        | 417/3125 [05:07<36:53,  1.22it/s]\n",
            " 13%|█▎        | 417/3125 [05:07<36:53,  1.22it/s]\n",
            " 13%|█▎        | 417/3125 [05:07<36:53,  1.22it/s]\n",
            " 13%|█▎        | 417/3125 [05:07<36:53,  1.22it/s]\n",
            " 13%|█▎        | 417/3125 [05:07<36:53,  1.22it/s]\n",
            " 13%|█▎        | 417/3125 [05:07<36:53,  1.22it/s]\n",
            " 13%|█▎        | 417/3125 [05:07<36:53,  1.22it/s]\n",
            " 13%|█▎        | 417/3125 [05:07<36:53,  1.22it/s]\n",
            " 13%|█▎        | 417/3125 [05:07<36:53,  1.22it/s]\n",
            " 13%|█▎        | 417/3125 [05:07<36:53,  1.22it/s]\n",
            " 13%|█▎        | 417/3125 [05:07<36:53,  1.22it/s]\n",
            " 13%|█▎        | 417/3125 [05:07<36:53,  1.22it/s]\n",
            " 13%|█▎        | 417/3125 [05:07<36:53,  1.22it/s]\n",
            " 13%|█▎        | 417/3125 [05:07<36:53,  1.22it/s]\n",
            " 13%|█▎        | 417/3125 [05:07<36:53,  1.22it/s]\n",
            "Map:   3%|▎         | 1578/50000 [00:00<00:03, 15670.58 examples/s]\n",
            " 13%|█▎        | 417/3125 [05:07<36:53,  1.22it/s]\n",
            " 13%|█▎        | 417/3125 [05:07<36:53,  1.22it/s]\n",
            " 13%|█▎        | 417/3125 [05:07<36:53,  1.22it/s]\n",
            " 13%|█▎        | 417/3125 [05:07<36:53,  1.22it/s]\n",
            " 13%|█▎        | 417/3125 [05:07<36:53,  1.22it/s]\n",
            " 13%|█▎        | 417/3125 [05:07<36:53,  1.22it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RayTaskError(ValueError)",
          "evalue": "\u001b[36mray::train_lora()\u001b[39m (pid=22919, ip=172.28.0.12)\n  File \"<ipython-input-28-8fa0fd3224c3>\", line 84, in train_lora\n  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/auto/auto_factory.py\", line 564, in from_pretrained\n    return model_class.from_pretrained(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\", line 4188, in from_pretrained\n    hf_quantizer.validate_environment(device_map=device_map)\n  File \"/usr/local/lib/python3.11/dist-packages/transformers/quantizers/quantizer_bnb_8bit.py\", line 101, in validate_environment\n    raise ValueError(\nValueError: Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the quantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules in 32-bit, you need to set `llm_int8_enable_fp32_cpu_offload=True` and pass a custom `device_map` to `from_pretrained`. Check https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu for more details.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRayTaskError(ValueError)\u001b[0m                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-8fa0fd3224c3>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;31m# Run LoRA Training on Multiple GPUs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_lora\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremote\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Adjust workers as needed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    136\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Final Training Losses from Parallel Runs:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlosses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ray/_private/auto_init_hook.py\u001b[0m in \u001b[0;36mauto_init_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mauto_init_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mauto_init_ray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mauto_init_wrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ray/_private/client_mode_hook.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    101\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"init\"\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mis_client_mode_enabled_by_default\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ray/_private/worker.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(object_refs, timeout)\u001b[0m\n\u001b[1;32m   2769\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2770\u001b[0m         \u001b[0;31m# TODO(ujvl): Consider how to allow user to retrieve the ready objects.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2771\u001b[0;31m         \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdebugger_breakpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mworker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_objects\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject_refs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2772\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2773\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRayError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ray/_private/worker.py\u001b[0m in \u001b[0;36mget_objects\u001b[0;34m(self, object_refs, timeout, return_exceptions, skip_deserialization)\u001b[0m\n\u001b[1;32m    917\u001b[0m                         \u001b[0mglobal_worker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore_worker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump_object_store_memory_usage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    918\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRayTaskError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 919\u001b[0;31m                         \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_instanceof_cause\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    920\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    921\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRayTaskError(ValueError)\u001b[0m: \u001b[36mray::train_lora()\u001b[39m (pid=22919, ip=172.28.0.12)\n  File \"<ipython-input-28-8fa0fd3224c3>\", line 84, in train_lora\n  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/auto/auto_factory.py\", line 564, in from_pretrained\n    return model_class.from_pretrained(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\", line 4188, in from_pretrained\n    hf_quantizer.validate_environment(device_map=device_map)\n  File \"/usr/local/lib/python3.11/dist-packages/transformers/quantizers/quantizer_bnb_8bit.py\", line 101, in validate_environment\n    raise ValueError(\nValueError: Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the quantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules in 32-bit, you need to set `llm_int8_enable_fp32_cpu_offload=True` and pass a custom `device_map` to `from_pretrained`. Check https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu for more details."
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Map: 100%|█████████▉| 49849/50000 [00:03<00:00, 16471.84 examples/s]\n",
            "Map: 100%|█████████▉| 49849/50000 [00:03<00:00, 16471.84 examples/s]\n",
            "Map: 100%|█████████▉| 49849/50000 [00:03<00:00, 16471.84 examples/s]\n",
            "Map: 100%|█████████▉| 49849/50000 [00:03<00:00, 16471.84 examples/s]\n",
            "Map: 100%|█████████▉| 49849/50000 [00:03<00:00, 16471.84 examples/s]\n",
            "Map: 100%|█████████▉| 49849/50000 [00:03<00:00, 16471.84 examples/s]"
          ]
        }
      ]
    }
  ]
}